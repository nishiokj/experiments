{"task_id":"swebench_astropy_astropy_12907","source":"swebench-lite","input":{"repo":"astropy/astropy","instance_id":"astropy__astropy-12907","base_commit":"d16bfe05a744909de4b27f5875fe0d4ed41ce607","prompt":"Modeling's `separability_matrix` does not compute separability correctly for nested CompoundModels Consider the following model: ```python from astropy.modeling import models as m from astropy.modeling.separable import separability_matrix cm = m.Linear1D(10) & m.Linear1D(5) ``` It's separability matrix as you might expect is a diagonal: ```python >>> separability_matrix(cm) array([[ True, False], [False, True]]) ``` If I make the model more complex: ```python >>> separability_matrix(m.Pix2Sky_TAN() & m.Linear1D(10) & m.Linear1D(5)) array([[ True, True, False, False], [ True, True, False, False], [False, False, True, False], [False, False, False, True]]) ``` The output matrix is again, as expected, the outputs and inputs to the linear models are separable and independent of each other. If however, I nest these compound models: ```python >>> separability_matrix(m.Pix2Sky_TAN() & cm) array([[ True, True, False, False], [ True, True, False, False], [False, False, True, True], [False, False, True, True]]) ``` Suddenly the inputs and outputs are no longer separable? This feels like a bug to me, but I might be missing something?"},"metadata":{"created_at":"2022-03-03T15:14:54Z","version":"4.3","environment_setup_commit":"298ccb478e6bf092953bca67a3d29dc6c35f6752"}}
{"task_id":"swebench_astropy_astropy_14182","source":"swebench-lite","input":{"repo":"astropy/astropy","instance_id":"astropy__astropy-14182","base_commit":"a5917978be39d13cd90b517e1de4e7a539ffaa48","prompt":"Please support header rows in RestructuredText output ### Description It would be great if the following would work: ```Python >>> from astropy.table import QTable >>> import astropy.units as u >>> import sys >>> tbl = QTable({'wave': [350,950]*u.nm, 'response': [0.7, 1.2]*u.count}) >>> tbl.write(sys.stdout, format=\"ascii.rst\") ===== ======== wave response ===== ======== 350.0 0.7 950.0 1.2 ===== ======== >>> tbl.write(sys.stdout, format=\"ascii.fixed_width\", header_rows=[\"name\", \"unit\"]) | wave | response | | nm | ct | | 350.0 | 0.7 | | 950.0 | 1.2 | >>> tbl.write(sys.stdout, format=\"ascii.rst\", header_rows=[\"name\", \"unit\"]) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python3/dist-packages/astropy/table/connect.py\", line 129, in __call__ self.registry.write(instance, *args, **kwargs) File \"/usr/lib/python3/dist-packages/astropy/io/registry/core.py\", line 369, in write return writer(data, *args, **kwargs) File \"/usr/lib/python3/dist-packages/astropy/io/ascii/connect.py\", line 26, in io_write return write(table, filename, **kwargs) File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 856, in write writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs) File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 800, in get_writer writer = core._get_writer(Writer, fast_writer, **kwargs) File \"/usr/lib/python3/dist-packages/astropy/io/ascii/core.py\", line 1719, in _get_writer writer = Writer(**writer_kwargs) TypeError: RST.__init__() got an unexpected keyword argument 'header_rows' ``` ### Additional context RestructuredText output is a great way to fill autogenerated documentation with content, so having this flexible makes the life easier `:-)`"},"metadata":{"created_at":"2022-12-16T11:13:37Z","version":"5.1","environment_setup_commit":"5f74eacbcc7fff707a44d8eb58adaa514cb7dcb5"}}
{"task_id":"swebench_astropy_astropy_14365","source":"swebench-lite","input":{"repo":"astropy/astropy","instance_id":"astropy__astropy-14365","base_commit":"7269fa3e33e8d02485a647da91a5a2a60a06af61","prompt":"ascii.qdp Table format assumes QDP commands are upper case ### Description ascii.qdp assumes that commands in a QDP file are upper case, for example, for errors they must be \"READ SERR 1 2\" whereas QDP itself is not case sensitive and case use \"read serr 1 2\". As many QDP files are created by hand, the expectation that all commands be all-caps should be removed. ### Expected behavior The following qdp file should read into a `Table` with errors, rather than crashing. ``` read serr 1 2 1 0.5 1 0.5 ``` ### How to Reproduce Create a QDP file: ``` > cat > test.qdp read serr 1 2 1 0.5 1 0.5 <EOF> > python Python 3.10.9 (main, Dec 7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> from astropy.table import Table >>> Table.read('test.qdp',format='ascii.qdp') WARNING: table_id not specified. Reading the first available table [astropy.io.ascii.qdp] Traceback (most recent call last): ... raise ValueError(f'Unrecognized QDP line: {line}') ValueError: Unrecognized QDP line: read serr 1 2 ``` Running \"qdp test.qdp\" works just fine. ### Versions Python 3.10.9 (main, Dec 7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)] astropy 5.1 Numpy 1.24.1 pyerfa 2.0.0.1 Scipy 1.10.0 Matplotlib 3.6.3","hints_text":"Welcome to Astropy ðŸ‘‹ and thank you for your first issue! A project member will respond to you as soon as possible; in the meantime, please double-check the [guidelines for submitting issues](https://github.com/astropy/astropy/blob/main/CONTRIBUTING.md#reporting-issues) and make sure you've provided the requested details. GitHub issues in the Astropy repository are used to track bug reports and feature requests; If your issue poses a question about how to use Astropy, please instead raise your question in the [Astropy Discourse user forum](https://community.openastronomy.org/c/astropy/8) and close this issue. If you feel that this issue has not been responded to in a timely manner, please send a message directly to the [development mailing list](http://groups.google.com/group/astropy-dev). If the issue is urgent or sensitive in nature (e.g., a security vulnerability) please send an e-mail directly to the private e-mail feedback@astropy.org. Huh, so we do have this format... https://docs.astropy.org/en/stable/io/ascii/index.html @taldcroft , you know anything about this? This is the format I'm using, which has the issue: https://docs.astropy.org/en/stable/api/astropy.io.ascii.QDP.html The issue is that the regex that searches for QDP commands is not case insensitive. This attached patch fixes the issue, but I'm sure there's a better way of doing it. [qdp.patch](https://github.com/astropy/astropy/files/10667923/qdp.patch) @jak574 - the fix is probably as simple as that. Would you like to put in a bugfix PR?"},"metadata":{"created_at":"2023-02-06T19:20:34Z","version":"5.1","environment_setup_commit":"5f74eacbcc7fff707a44d8eb58adaa514cb7dcb5"}}
{"task_id":"swebench_astropy_astropy_14995","source":"swebench-lite","input":{"repo":"astropy/astropy","instance_id":"astropy__astropy-14995","base_commit":"b16c7d12ccbc7b2d20364b89fb44285bcbfede54","prompt":"In v5.3, NDDataRef mask propagation fails when one of the operand does not have a mask ### Description This applies to v5.3. It looks like when one of the operand does not have a mask, the mask propagation when doing arithmetic, in particular with `handle_mask=np.bitwise_or` fails. This is not a problem in v5.2. I don't know enough about how all that works, but it seems from the error that the operand without a mask is set as a mask of None's and then the bitwise_or tries to operate on an integer and a None and fails. ### Expected behavior When one of the operand does not have mask, the mask that exists should just be copied over to the output. Or whatever was done in that situation in v5.2 where there's no problem. ### How to Reproduce This is with v5.3. With v5.2, there are no errors. ``` >>> import numpy as np >>> from astropy.nddata import NDDataRef >>> array = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]) >>> mask = np.array([[0, 1, 64], [8, 0, 1], [2, 1, 0]]) >>> nref_nomask = NDDataRef(array) >>> nref_mask = NDDataRef(array, mask=mask) # multiply no mask by constant (no mask * no mask) >>> nref_nomask.multiply(1., handle_mask=np.bitwise_or).mask # returns nothing, no mask, OK # multiply no mask by itself (no mask * no mask) >>> nref_nomask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask # return nothing, no mask, OK # multiply mask by constant (mask * no mask) >>> nref_mask.multiply(1., handle_mask=np.bitwise_or).mask ... TypeError: unsupported operand type(s) for |: 'int' and 'NoneType' # multiply mask by itself (mask * mask) >>> nref_mask.multiply(nref_mask, handle_mask=np.bitwise_or).mask array([[ 0, 1, 64], [ 8, 0, 1], [ 2, 1, 0]]) # multiply mask by no mask (mask * no mask) >>> nref_mask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask ... TypeError: unsupported operand type(s) for |: 'int' and 'NoneType' ``` ### Versions >>> import sys; print(\"Python\", sys.version) Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ] >>> import astropy; print(\"astropy\", astropy.__version__) astropy 5.3 >>> import numpy; print(\"Numpy\", numpy.__version__) Numpy 1.24.3 >>> import erfa; print(\"pyerfa\", erfa.__version__) pyerfa 2.0.0.3 >>> import scipy; print(\"Scipy\", scipy.__version__) Scipy 1.10.1 >>> import matplotlib; print(\"Matplotlib\", matplotlib.__version__) Matplotlib 3.7.1","hints_text":"Welcome to Astropy ðŸ‘‹ and thank you for your first issue! A project member will respond to you as soon as possible; in the meantime, please double-check the [guidelines for submitting issues](https://github.com/astropy/astropy/blob/main/CONTRIBUTING.md#reporting-issues) and make sure you've provided the requested details. GitHub issues in the Astropy repository are used to track bug reports and feature requests; If your issue poses a question about how to use Astropy, please instead raise your question in the [Astropy Discourse user forum](https://community.openastronomy.org/c/astropy/8) and close this issue. If you feel that this issue has not been responded to in a timely manner, please send a message directly to the [development mailing list](http://groups.google.com/group/astropy-dev). If the issue is urgent or sensitive in nature (e.g., a security vulnerability) please send an e-mail directly to the private e-mail feedback@astropy.org. @bmorris3 , do you think this is related to that nddata feature you added in v5.3? Hi @KathleenLabrie. I'm not sure this is a bug, because as far as I can tell the `mask` in NDData is assumed to be boolean: https://github.com/astropy/astropy/blob/83f6f002fb11853eacb689781d366be6aa170e0e/astropy/nddata/nddata.py#L51-L55 There are updates to the propagation logic in v5.3 that allow for more flexible and customizable mask propagation, see discussion in https://github.com/astropy/astropy/pull/14175. You're using the `bitwise_or` operation, which is different from the default `logical_or` operation in important ways. I tested your example using `logical_or` and it worked as expected, with the caveat that your mask becomes booleans with `True` for non-zero initial mask values. We are doing data reduction. The nature of the \"badness\" of each pixel matters. True or False does not cut it. That why we need bits. This is scientifically required. A saturated pixel is different from a non-linear pixel, different from an unilliminated pixels, different .... etc. I don't see why a feature that had been there for a long time was removed without even a deprecation warning. BTW, I still think that something is broken, eg. ``` >>> bmask = np.array([[True, False, False], [False, True, False], [False, False, True]]) >>> nref_bmask = NDDataRef(array, mask=bmask) >>> nref_bmask.multiply(1.).mask array([[True, None, None], [None, True, None], [None, None, True]], dtype=object) ``` Those `None`s should probably be `False`s not None's There is *absolutely* a bug here. Here's a demonstration: ``` >>> data = np.arange(4).reshape(2,2) >>> mask = np.array([[1, 0], [0, 1]])) >>> nd1 = NDDataRef(data, mask=mask) >>> nd2 = NDDataRef(data, mask=None) >>> nd1.multiply(nd2, handle_mask=np.bitwise_or) ...Exception... >>> nd2.multiply(nd1, handle_mask=np.bitwise_or) NDDataRef([[0, 1], [4, 9]]) ``` Multiplication is commutative and should still be here. In 5.2 the logic for arithmetic between two objects was that if one didn't have a `mask` or the `mask` was `None` then the output mask would be the `mask` of the other. That seems entirely sensible and I see no sensible argument for changing that. But in 5.3 the logic is that if the first operand has no mask then the output will be the mask of the second, but if the second operand has no mask then it sends both masks to the `handle_mask` function (instead of simply setting the output to the mask of the first as before). Note that this has an unwanted effect *even if the masks are boolean*: ``` >>> bool_mask = mask.astype(bool) >>> nd1 = NDDataRef(data, mask=bool_mask) >>> nd2.multiply(nd1).mask array([[False, True], [ True, False]]) >>> nd1.multiply(nd2).mask array([[None, True], [True, None]], dtype=object) ``` and, whoops, the `mask` isn't a nice happy numpy `bool` array anymore. So it looks like somebody accidentally turned the lines ``` elif operand.mask is None: return deepcopy(self.mask) ``` into ``` elif operand is None: return deepcopy(self.mask) ``` @chris-simpson I agree that line you suggested above is the culprit, which was [changed here](https://github.com/astropy/astropy/commit/feeb716b7412c477c694648ee1e93be2c4a73700#diff-5057de973eaa1e5036a0bef89e618b1b03fd45a9c2952655abb656822f4ddc2aL458-R498). I've reverted that specific line in a local astropy branch and verified that the existing tests still pass, and the bitmask example from @KathleenLabrie works after that line is swapped. I'll make a PR to fix this today, with a new test to make sure that we don't break this again going forward. Many thanks for working on this, @bmorris3. Regarding whether the `mask` is assumed to be Boolean, I had noticed in the past that some developers understood this to be the case, while others disagreed. When we discussed this back in 2016, however (as per the document you linked to in Slack), @eteq explained that the mask is just expected to be \"truthy\" in a NumPy sense of zero = False (unmasked) and non-zero = True (masked), which you'll see is consistent with the doc string you cited above, even if it's not entirely clear :slightly_frowning_face:. Of course I think that flexibility is great, but I think intentional ambiguity in docs is risky when only one of the two cases is tested. ðŸ˜¬ Indeed, I should probably have checked that there was a test for this upstream, since I was aware of some confusion; if only we could find more time to work on these important common bits that we depend on..."},"metadata":{"created_at":"2023-06-27T19:48:18Z","version":"5.2","environment_setup_commit":"362f6df12abf9bd769d4915fabf955c993ea22cf"}}
{"task_id":"swebench_astropy_astropy_6938","source":"swebench-lite","input":{"repo":"astropy/astropy","instance_id":"astropy__astropy-6938","base_commit":"c76af9ed6bb89bfba45b9f5bc1e635188278e2fa","prompt":"Possible bug in io.fits related to D exponents I came across the following code in ``fitsrec.py``: ```python # Replace exponent separator in floating point numbers if 'D' in format: output_field.replace(encode_ascii('E'), encode_ascii('D')) ``` I think this may be incorrect because as far as I can tell ``replace`` is not an in-place operation for ``chararray`` (it returns a copy). Commenting out this code doesn't cause any tests to fail so I think this code isn't being tested anyway.","hints_text":"It is tested with `astropy/io/fits/tests/test_checksum.py:test_ascii_table_data` but indeed the operation is not inplace and it does not fail. Using 'D' is probably better, but since #5362 (I had vague memory about something like this ^^, see also #5353) anyway 'D' and 'E' are read as double, so I think there is not difference on Astropy side."},"metadata":{"created_at":"2017-12-07T00:01:14Z","version":"1.3","environment_setup_commit":"848c8fa21332abd66b44efe3cb48b72377fb32cc"}}
{"task_id":"swebench_astropy_astropy_7746","source":"swebench-lite","input":{"repo":"astropy/astropy","instance_id":"astropy__astropy-7746","base_commit":"d5bd3f68bb6d5ce3a61bdce9883ee750d1afade5","prompt":"Issue when passing empty lists/arrays to WCS transformations The following should not fail but instead should return empty lists/arrays: ``` In [1]: from astropy.wcs import WCS In [2]: wcs = WCS('2MASS_h.fits') In [3]: wcs.wcs_pix2world([], [], 0) --------------------------------------------------------------------------- InconsistentAxisTypesError Traceback (most recent call last) <ipython-input-3-e2cc0e97941a> in <module>() ----> 1 wcs.wcs_pix2world([], [], 0) ~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in wcs_pix2world(self, *args, **kwargs) 1352 return self._array_converter( 1353 lambda xy, o: self.wcs.p2s(xy, o)['world'], -> 1354 'output', *args, **kwargs) 1355 wcs_pix2world.__doc__ = \"\"\" 1356 Transforms pixel coordinates to world coordinates by doing ~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _array_converter(self, func, sky, ra_dec_order, *args) 1267 \"a 1-D array for each axis, followed by an origin.\") 1268 -> 1269 return _return_list_of_arrays(axes, origin) 1270 1271 raise TypeError( ~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _return_list_of_arrays(axes, origin) 1223 if ra_dec_order and sky == 'input': 1224 xy = self._denormalize_sky(xy) -> 1225 output = func(xy, origin) 1226 if ra_dec_order and sky == 'output': 1227 output = self._normalize_sky(output) ~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in <lambda>(xy, o) 1351 raise ValueError(\"No basic WCS settings were created.\") 1352 return self._array_converter( -> 1353 lambda xy, o: self.wcs.p2s(xy, o)['world'], 1354 'output', *args, **kwargs) 1355 wcs_pix2world.__doc__ = \"\"\" InconsistentAxisTypesError: ERROR 4 in wcsp2s() at line 2646 of file cextern/wcslib/C/wcs.c: ncoord and/or nelem inconsistent with the wcsprm. ```"},"metadata":{"created_at":"2018-08-20T14:07:20Z","version":"1.3","environment_setup_commit":"848c8fa21332abd66b44efe3cb48b72377fb32cc"}}
{"task_id":"swebench_django_django_10914","source":"swebench-lite","input":{"repo":"django/django","instance_id":"django__django-10914","base_commit":"e7fd69d051eaa67cb17f172a39b57253e9cb831a","prompt":"Set default FILE_UPLOAD_PERMISSION to 0o644. Description Hello, As far as I can see, the â€‹File Uploads documentation page does not mention any permission issues. What I would like to see is a warning that in absence of explicitly configured FILE_UPLOAD_PERMISSIONS, the permissions for a file uploaded to FileSystemStorage might not be consistent depending on whether a MemoryUploadedFile or a TemporaryUploadedFile was used for temporary storage of the uploaded data (which, with the default FILE_UPLOAD_HANDLERS, in turn depends on the uploaded data size). The tempfile.NamedTemporaryFile + os.rename sequence causes the resulting file permissions to be 0o0600 on some systems (I experience it here on CentOS 7.4.1708 and Python 3.6.5). In all probability, the implementation of Python's built-in tempfile module explicitly sets such permissions for temporary files due to security considerations. I found mentions of this issue â€‹on GitHub, but did not manage to find any existing bug report in Django's bug tracker.","hints_text":"I think you're talking about ef70af77ec53160d5ffa060c1bdf5ed93322d84f (#28540). I guess the question is whether or not that documentation should be duplicated elsewhere. Thank you Tim, this is precisely what I was looking for! I can only see one issue with the current docs (if you excuse me for bothering you with such minor details). â€‹The documentation for the FILE_UPLOAD_PERMISSIONS setting reads: If this isnâ€™t given or is None, youâ€™ll get operating-system dependent behavior. On most platforms, temporary files will have a mode of 0o600, and files saved from memory will be saved using the systemâ€™s standard umask. As I would understand this text, only temporary files get a mode of 0o600. I would then ask myself: \"Why should I care about temporary files, they should be gone anyway after the file is uploaded?\" and skip setting FILE_UPLOAD_PERMISSIONS. What is important but is not properly conveyed to the user is that not only temporary files themselves, but also the actual files which end up in the media folder get permissions of 0o600. Currently a developer can only discover this either by careful reading of the Deployment checklist page (manage.py check --deploy does not seem to check FILE_UPLOAD_PERMISSIONS) or by hitting the inconsistent permissions accidentally (like I did). I propose to unify the docs for FILE_UPLOAD_PERMISSIONS on the Settings page and the Deployment checklist page like this: â€‹https://gist.github.com/earshinov/0340f741189a14d4fd10e3e902203ad6/revisions#diff-14151589d5408f8b64b7e0e580770f0e Pros: It makes more clear that one gets different permissions for the *uploaded* files. It makes the docs more unified and thus easier to synchronously change in the future if/when required. I recognize that my edits might seem too minor and insignificant to be worth the hassle of editing the docs, committing, re-publishing them etc., but still I hope you will find them useful enough to be integrated into the official docs. Now that I think about, maybe Django could provide # <Commentary about inconsistent permissions when this setting is omitted> FILE_UPLOAD_PERMISSINS=0o600 in the â€‹default project settings so that developers don't miss it? 600 seems a reasonable default, particularly because people would get 600 anyway (at least on some operating systems) when the TemporaryFileUploadHandler is engaged. Since this has come up again, I've suggested on django-developers (â€‹https://groups.google.com/d/topic/django-developers/h9XbQAPv5-I/discussion) that we adjust the FILE_UPLOAD_PERMISSION default to 0o644 (This was the conclusion I eventually came to from the discussion on #28540.) Lets see what people say there. Thus far, no great objections on the mailing list to adjusting the FILE_UPLOAD_PERMISSION default. Thus I'm going to rename this and Accept on that basis. A PR would need to: Adjust the default. Add a Breaking Change note to releases/2.2.txt (on the assumption we can get it in for then.) â€” This should include a set to None to restore previous behaviour' type comment. Adjust the references in the settings docs and deployment checklist. Make sure any other references are adjusted. Replying to Carlton Gibson: Thus far, no great objections on the mailing list to adjusting the FILE_UPLOAD_PERMISSION default. Thus I'm going to rename this and Accept on that basis. Thank you! Hopefully, this change will prevent confusion and unpleasant surprises for Django users in the future. Hello everyone, I would like to work on this. But before that there are few important questions: There is a related setting called FILE_UPLOAD_DIRECTORY_PERMISSIONS. Its document says that This value mirrors the functionality and caveats of the FILE_UPLOAD_PERMISSIONS setting. Shall we also change its default from None to 0o644(Please suggest if something different should be provided for directories) and update its document as well? Since 2.2 pre-release branch is now in feature freeze state, Shall we move the change to 3.0 version? On a side note, some tests must be refactored for new values for both of these settings. I think that's alright. That note is referring to that non-leaf directories are created using the process umask. (See â€‹`makedirs()` docs.) This is similar to FILE_UPLOAD_PERMISSIONS, when not using the temporary file upload handler. The underlying issue here is the inconsistency in file permissions, depending on the file size, when using the default settings that Django provides. There is no such inconsistency with directory permissions. As such changes should not be needed to FILE_UPLOAD_DIRECTORY_PERMISSIONS. (Any issues there would need to be addressed under a separate ticket.) Replying to Carlton Gibson: I see and understand the issue better now. Thanks for the clarification. I'll make the changes as you have suggested in your previous comment. Only question remaining is about introducing this change in 3.0 version. Shall we move it to 3.0 release? Shall we move it to 3.0 release? Yes please."},"metadata":{"created_at":"2019-01-30T13:13:20Z","version":"3.0","environment_setup_commit":"419a78300f7cd27611196e1e464d50fd0385ff27"}}
{"task_id":"swebench_django_django_10924","source":"swebench-lite","input":{"repo":"django/django","instance_id":"django__django-10924","base_commit":"bceadd2788dc2dad53eba0caae172bd8522fd483","prompt":"Allow FilePathField path to accept a callable. Description I have a special case where I want to create a model containing the path to some local files on the server/dev machine. Seeing as the place where these files are stored is different on different machines I have the following: import os from django.conf import settings from django.db import models class LocalFiles(models.Model): name = models.CharField(max_length=255) file = models.FilePathField(path=os.path.join(settings.LOCAL_FILE_DIR, 'example_dir')) Now when running manage.py makemigrations it will resolve the path based on the machine it is being run on. Eg: /home/<username>/server_files/example_dir I had to manually change the migration to include the os.path.join() part to not break this when running the migration on production/other machine.","hints_text":"So, to clarify, what exactly is the bug/feature proposal/issue here? The way I see it, you're supposed to use os.path.join() and LOCAL_FILE_DIR to define a relative path. It's sort of like how we use BASE_DIR to define relative paths in a lot of other places. Could you please clarify a bit more as to what the issue is so as to make it easier to test and patch? Replying to Hemanth V. Alluri: So, to clarify, what exactly is the bug/feature proposal/issue here? The way I see it, you're supposed to use os.path.join() and LOCAL_FILE_DIR to define a relative path. It's sort of like how we use BASE_DIR to define relative paths in a lot of other places. Could you please clarify a bit more as to what the issue is so as to make it easier to test and patch? LOCAL_FILE_DIR doesn't have to be the same on another machine, and in this case it isn't the same on the production server. So the os.path.join() will generate a different path on my local machine compared to the server. When i ran ./manage.py makemigrations the Migration had the path resolved \"hardcoded\" to my local path, which will not work when applying that path on the production server. This will also happen when using the BASE_DIR setting as the path of your FilePathField, seeing as that's based on the location of your project folder, which will almost always be on a different location. My suggestion would be to let makemigrations not resolve the path and instead keep the os.path.join(), which I have now done manually. More importantly would be to retain the LOCAL_FILE_DIR setting in the migration. Replying to Sebastiaan Arendsen: Replying to Hemanth V. Alluri: So, to clarify, what exactly is the bug/feature proposal/issue here? The way I see it, you're supposed to use os.path.join() and LOCAL_FILE_DIR to define a relative path. It's sort of like how we use BASE_DIR to define relative paths in a lot of other places. Could you please clarify a bit more as to what the issue is so as to make it easier to test and patch? LOCAL_FILE_DIR doesn't have to be the same on another machine, and in this case it isn't the same on the production server. So the os.path.join() will generate a different path on my local machine compared to the server. When i ran ./manage.py makemigrations the Migration had the path resolved \"hardcoded\" to my local path, which will not work when applying that path on the production server. This will also happen when using the BASE_DIR setting as the path of your FilePathField, seeing as that's based on the location of your project folder, which will almost always be on a different location. My suggestion would be to let makemigrations not resolve the path and instead keep the os.path.join(), which I have now done manually. More importantly would be to retain the LOCAL_FILE_DIR setting in the migration. Please look at this ticket: https://code.djangoproject.com/ticket/6896 I think that something like what sandychapman suggested about an extra flag would be cool if the design decision was approved and if there were no restrictions in the implementation for such a change to be made. But that's up to the developers who have had more experience with the project to decide, not me. This seems a reasonable use-case: allow FilePathField to vary path by environment. The trouble with os.path.join(...) is that it will always be interpreted at import time, when the class definition is loaded. (The (...) say, ...and call this....) The way to defer that would be to all path to accept a callable, similarly to how FileField's upload_to takes a callable. It should be enough to evaluate the callable in FilePathField.__init__(). Experimenting with generating a migration looks good. (The operation gives path the fully qualified import path of the specified callable, just as with upload_to.) I'm going to tentatively mark this as Easy Pickings: it should be simple enough. Replying to Nicolas NoÃ©: Hi Nicolas, Are you still working on this ticket? Sorry, I forgot about it. I'll try to solve this real soon (or release the ticket if I can't find time for it). â€‹PR Can I work on this ticket ? Sure, sorry for blocking the ticket while I was too busy... I think that Nicolas Noe's solution, â€‹PR, was correct. The model field can accept a callable as it is currently implemented. If you pass it a callable for the path argument it will correctly use that fully qualified function import path in the migration. The problem is when you go to actually instantiate a FilePathField instance, the FilePathField form does some type checking and gives you one of these TypeError: scandir: path should be string, bytes, os.PathLike or None, not function This can be avoided by evaluating the path function first thing in the field form __init__ function, as in the pull request. Then everything seems to work fine. Hi, If I only change self.path in forms/fields.py, right after __init__ I get this error: File \"/home/hpfn/Documentos/Programacao/python/testes/.venv/lib/python3.6/site-packages/django/forms/fields.py\", line 1106, in __init__ self.choices.append((f, f.replace(path, \"\", 1))) TypeError: replace() argument 1 must be str, not function The 'path' param is used a few lines after. There is one more time. Line 1106 can be wrong. If I put in models/fields/__init__.py - after super(): if callable(self.path): self.path = self.path() I can run 'python manage.py runserver' It can be: if callable(path): path = path() at the beginning of forms/fields.py â€‹PR All comments in the original PR (â€‹https://github.com/django/django/pull/10299/commits/7ddb83ca7ed5b2a586e9d4c9e0a79d60b27c26b6) seems to be resolved in the latter one (â€‹https://github.com/django/django/pull/10924/commits/9c3b2c85e46efcf1c916e4b76045d834f16050e3). Any hope of this featuring coming through. Django keep bouncing between migrations due to different paths to models.FilePathField"},"metadata":{"created_at":"2019-02-03T11:30:12Z","version":"3.0","environment_setup_commit":"419a78300f7cd27611196e1e464d50fd0385ff27"}}
{"task_id":"swebench_django_django_11001","source":"swebench-lite","input":{"repo":"django/django","instance_id":"django__django-11001","base_commit":"ef082ebb84f00e38af4e8880d04e8365c2766d34","prompt":"Incorrect removal of order_by clause created as multiline RawSQL Description Hi. The SQLCompiler is ripping off one of my \"order by\" clause, because he \"thinks\" the clause was already \"seen\" (in SQLCompiler.get_order_by()). I'm using expressions written as multiline RawSQLs, which are similar but not the same. The bug is located in SQLCompiler.get_order_by(), somewhere around line computing part of SQL query without ordering: without_ordering = self.ordering_parts.search(sql).group(1) The sql variable contains multiline sql. As a result, the self.ordering_parts regular expression is returning just a line containing ASC or DESC words. This line is added to seen set, and because my raw queries have identical last lines, only the first clasue is returing from SQLCompiler.get_order_by(). As a quick/temporal fix I can suggest making sql variable clean of newline characters, like this: sql_oneline = ' '.join(sql.split('\\n')) without_ordering = self.ordering_parts.search(sql_oneline).group(1) Note: beware of unicode (Py2.x u'') and EOL dragons (\\r). Example of my query: return MyModel.objects.all().order_by( RawSQL(''' case when status in ('accepted', 'verification') then 2 else 1 end''', []).desc(), RawSQL(''' case when status in ('accepted', 'verification') then (accepted_datetime, preferred_datetime) else null end''', []).asc(), RawSQL(''' case when status not in ('accepted', 'verification') then (accepted_datetime, preferred_datetime, created_at) else null end''', []).desc()) The ordering_parts.search is returing accordingly: ' then 2 else 1 end)' ' else null end' ' else null end' Second RawSQL with a else null end part is removed from query. The fun thing is that the issue can be solved by workaround by adding a space or any other char to the last line. So in case of RawSQL I can just say, that current implementation of avoiding duplicates in order by clause works only for special/rare cases (or does not work in all cases). The bug filed here is about wrong identification of duplicates (because it compares only last line of SQL passed to order by clause). Hope my notes will help you fixing the issue. Sorry for my english.","hints_text":"Is there a reason you can't use â€‹conditional expressions, e.g. something like: MyModel.objects.annotate( custom_order=Case( When(...), ) ).order_by('custom_order') I'm thinking that would avoid fiddly ordering_parts regular expression. If there's some shortcoming to that approach, it might be easier to address that. Allowing the ordering optimization stuff to handle arbitrary RawSQL may be difficult. Is there a reason you can't use â€‹conditional expressions No, but I didn't knew about the issue, and writing raw sqls is sometimes faster (not in this case ;) I'm really happy having possibility to mix raw sqls with object queries. Next time I'll use expressions, for sure. Allowing the ordering optimization stuff to handle arbitrary RawSQL may be difficult. Personally I'd like to skip RawSQL clauses in the block which is responsible for finding duplicates. If someone is using raw sqls, he knows the best what he is doing, IMO. And it is quite strange if Django removes silently part of your SQL. This is very confusing. And please note that printing a Query instance was generating incomplete sql, but while checking Query.order_by manually, the return value was containing all clauses. I thought that just printing was affected, but our QA dept told me the truth ;) I know there is no effective way to compare similarity of two raw clauses. This may be hard for expression objects, too, but you have a possibility to implement some __eq__ magic (instead of comparation of generated sqls). Unfortunately I don't know why duplicates detection was implemented, so it's hard to tell how to improve this part. Patches welcome, I suppose. â€‹PR Is there a reason why you didn't add tests? I was waiting for confirmation, I've added a test. Is it enough? Some additional test coverage needed."},"metadata":{"created_at":"2019-02-17T13:02:09Z","version":"3.0","environment_setup_commit":"419a78300f7cd27611196e1e464d50fd0385ff27"}}
{"task_id":"swebench_django_django_11019","source":"swebench-lite","input":{"repo":"django/django","instance_id":"django__django-11019","base_commit":"93e892bb645b16ebaf287beb5fe7f3ffe8d10408","prompt":"Merging 3 or more media objects can throw unnecessary MediaOrderConflictWarnings Description Consider the following form definition, where text-editor-extras.js depends on text-editor.js but all other JS files are independent: from django import forms class ColorPicker(forms.Widget): class Media: js = ['color-picker.js'] class SimpleTextWidget(forms.Widget): class Media: js = ['text-editor.js'] class FancyTextWidget(forms.Widget): class Media: js = ['text-editor.js', 'text-editor-extras.js', 'color-picker.js'] class MyForm(forms.Form): background_color = forms.CharField(widget=ColorPicker()) intro = forms.CharField(widget=SimpleTextWidget()) body = forms.CharField(widget=FancyTextWidget()) Django should be able to resolve the JS files for the final form into the order text-editor.js, text-editor-extras.js, color-picker.js. However, accessing MyForm().media results in: /projects/django/django/forms/widgets.py:145: MediaOrderConflictWarning: Detected duplicate Media files in an opposite order: text-editor-extras.js text-editor.js MediaOrderConflictWarning, Media(css={}, js=['text-editor-extras.js', 'color-picker.js', 'text-editor.js']) The MediaOrderConflictWarning is a result of the order that the additions happen in: ColorPicker().media + SimpleTextWidget().media produces Media(css={}, js=['color-picker.js', 'text-editor.js']), which (wrongly) imposes the constraint that color-picker.js must appear before text-editor.js. The final result is particularly unintuitive here, as it's worse than the \"naÃ¯ve\" result produced by Django 1.11 before order-checking was added (color-picker.js, text-editor.js, text-editor-extras.js), and the pair of files reported in the warning message seems wrong too (aren't color-picker.js and text-editor.js the wrong-ordered ones?)","hints_text":"As a tentative fix, I propose that media objects should explicitly distinguish between cases where we do / don't care about ordering, notionally something like: class FancyTextWidget(forms.Widget): class Media: js = { ('text-editor.js', 'text-editor-extras.js'), # tuple = order is important 'color-picker.js' # set = order is unimportant } (although using a set for this is problematic due to the need for contents to be hashable), and the result of adding two media objects should be a \"don't care\" so that we aren't introducing dependencies where the original objects didn't have them. We would then defer assembling them into a flat list until the final render call. I haven't worked out the rest of the algorithm yet, but I'm willing to dig further if this sounds like a sensible plan of attack... Are you testing with the fix from #30153? Yes, testing against current master (b39bd0aa6d5667d6bbcf7d349a1035c676e3f972). So â€‹https://github.com/django/django/commit/959d0c078a1c903cd1e4850932be77c4f0d2294d (the fix for #30153) didn't make this case worse, it just didn't improve on it. The problem is actually the same I encountered, with the same unintuitive error message too. There is still a way to produce a conflicting order but it's harder to trigger in the administration interface now but unfortunately still easy. Also, going back to the state of things pre 2.0 was already discussed previously and rejected. Here's a failing test and and an idea to make this particular test pass: Merge the JS sublists starting from the longest list and continuing with shorter lists. The CSS case is missing yet. The right thing to do would be (against â€‹worse is better) to add some sort of dependency resolution solver with backtracking but that's surely a bad idea for many other reasons. The change makes some old tests fail (I only took a closer look at test_merge_js_three_way and in this case the failure is fine -- custom_widget.js is allowed to appear before jquery.js.) diff --git a/django/forms/widgets.py b/django/forms/widgets.py index 02aa32b207..d85c409152 100644 --- a/django/forms/widgets.py +++ b/django/forms/widgets.py @@ -70,9 +70,15 @@ class Media: @property def _js(self): - js = self._js_lists[0] + sorted_by_length = list(sorted( + filter(None, self._js_lists), + key=lambda lst: -len(lst), + )) + if not sorted_by_length: + return [] + js = sorted_by_length[0] # filter(None, ...) avoids calling merge() with empty lists. - for obj in filter(None, self._js_lists[1:]): + for obj in filter(None, sorted_by_length[1:]): js = self.merge(js, obj) return js diff --git a/tests/forms_tests/tests/test_media.py b/tests/forms_tests/tests/test_media.py index 8cb484a15e..9d17ad403b 100644 --- a/tests/forms_tests/tests/test_media.py +++ b/tests/forms_tests/tests/test_media.py @@ -571,3 +571,12 @@ class FormsMediaTestCase(SimpleTestCase): # was never specified. merged = widget3 + form1 + form2 self.assertEqual(merged._css, {'screen': ['a.css', 'b.css'], 'all': ['c.css']}) + + def test_merge_js_some_more(self): + widget1 = Media(js=['color-picker.js']) + widget2 = Media(js=['text-editor.js']) + widget3 = Media(js=['text-editor.js', 'text-editor-extras.js', 'color-picker.js']) + + merged = widget1 + widget2 + widget3 + + self.assertEqual(merged._js, ['text-editor.js', 'text-editor-extras.js', 'color-picker.js']) Thinking some more: sorted() is more likely to break existing code because people probably haven't listed all dependencies in their js attributes now. Yes, that's not what they should have done, but breaking peoples' projects sucks and I don't really want to do that (even if introducing sorted() might be the least disruptive and at the same time most correct change) wanting to handle the jquery, widget1, noConflict and jquery, widget2, noConflict case has introduced an unexpected amount of complexity introducing a complex solving framework will have a really bad impact on runtime and will introduce even more complexity and is out of the question to me I'm happy to help fixing this but right now I only see bad and worse choices. I don't think sorting by length is the way to go - it would be trivial to make the test fail again by extending the first list with unrelated items. It might be a good real-world heuristic for finding a solution more often, but that's just trading a reproducible bug for an unpredictable one. (I'm not sure I'd trust it as a heuristic either: we've encountered this issue on Wagtail CMS, where we're making extensive use of form media on hierarchical form structures, and so those media definitions will tend to bubble up several layers to reach the top level. At that point, there's no way of knowing whether the longer list is the one with more complex dependencies, or just one that collected more unrelated files on the way up the tree...) I'll do some more thinking on this. My hunch is that even if it does end up being a travelling-salesman-type problem, it's unlikely to be run on a large enough data set for performance to be an issue. I don't think sorting by length is the way to go - it would be trivial to make the test fail again by extending the first list with unrelated items. It might be a good real-world heuristic for finding a solution more often, but that's just trading a reproducible bug for an unpredictable one. Well yes, if the ColorPicker itself would have a longer list of JS files it depends on then it would fail too. If, on the other hand, it wasn't a ColorPicker widget but a ColorPicker formset or form the initially declared lists would still be preserved and sorting the lists by length would give the correct result. Since #30153 the initially declared lists (or tuples) are preserved so maybe you have many JS and CSS declarations but as long as they are unrelated there will not be many long sublists. I'm obviously happy though if you're willing to spend the time finding a robust solution to this problem. (For the record: Personally I was happy with the state of things pre-2.0 too... and For the record 2: I'm also using custom widgets and inlines in feincms3/django-content-editor. It's really surprising to me that we didn't stumble on this earlier since we're always working on the latest Django version or even on pre-release versions if at all possible) Hi there, I'm the dude who implemented the warning. I am not so sure this is a bug. Let's try tackle this step by step. The new merging algorithm that was introduced in version 2 is an improvement. It is the most accurate way to merge two sorted lists. It's not the simplest way, but has been reviewed plenty times. The warning is another story. It is independent from the algorithm. It merely tells you that the a certain order could not be maintained. We figured back than, that this would be a good idea. It warns a developer about a potential issue, but does not raise an exception. With that in mind, the correct way to deal with the issue described right now, is to ignore the warning. BUT, that doesn't mean that you don't have a valid point. There are implicit and explicit orders. Not all assets require ordering and (random) orders that only exist because of Media merging don't matter at all. This brings me back to a point that I have [previously made](https://code.djangoproject.com/ticket/30153#comment:6). It would make sense to store the original lists, which is now the case on master, and only raise if the order violates the original list. The current implementation on master could also be improved by removing duplicates. Anyways, I would considers those changes improvements, but not bug fixes. I didn't have time yet to look into this. But I do have some time this weekend. If you want I can take another look into this and propose a solution that solves this issue. Best -Joe \"Ignore the warning\" doesn't work here - the order-fixing has broken the dependency between text-editor.js and text-editor-extras.js. I can (reluctantly) accept an implementation that produces false warnings, and I can accept that a genuine dependency loop might produce undefined behaviour, but the combination of the two - breaking the ordering as a result of seeing a loop that isn't there - is definitely a bug. (To be clear, I'm not suggesting that the 2.x implementation is a step backwards from not doing order checking at all - but it does introduce a new failure case, and that's what I'm keen to fix.) To summarise: Even with the new strategy in #30153 of holding on to the un-merged lists as long as possible, the final merging is still done by adding one list at a time. The intermediate results are lists, which are assumed to be order-critical; this means the intermediate results have additional constraints that are not present in the original lists, causing it to see conflicts where there aren't any. Additionally, we should try to preserve the original sequence of files as much as possible, to avoid unnecessarily breaking user code that hasn't fully specified its dependencies and is relying on the 1.x behaviour. I think we need to approach this as a graph problem (which I realise might sound like overkill, but I'd rather start with something formally correct and optimise later as necessary): a conflict occurs whenever the dependency graph is cyclic. #30153 is a useful step towards this, as it ensures we have the accurate dependency graph up until the point where we need to assemble the final list. I suggest we replace Media.merge with a new method that accepts any number of lists (using *args if we want to preserve the existing method signature for backwards compatibility). This would work as follows: Iterate over all items in all sub-lists, building a dependency graph (where a dependency is any item that immediately precedes it within a sub-list) and a de-duplicated list containing all items indexed in the order they are first encountered Starting from the first item in the de-duplicated list, backtrack through the dependency graph, following the lowest-indexed dependency each time until we reach an item with no dependencies. While backtracking, maintain a stack of visited items. If we encounter an item already on the stack, this is a dependency loop; throw a MediaOrderConflictWarning and break out of the backtracking loop Output the resulting item, then remove it from the dependency graph and the de-duplicated list If the 'visited items' stack is non-empty, pop the last item off it and repeat the backtracking step from there. Otherwise, repeat the backtracking step starting from the next item in the de-duplicated list Repeat until no items remain This sounds correct. I'm not sure it's right though. It does sound awfully complex for what there is to gain. Maintaining this down the road will not get easier. Finding, explaining and understanding the fix for #30153 did already cost a lot of time which could also have been invested elsewhere. If I manually assign widget3's JS lists (see https://code.djangoproject.com/ticket/30179#comment:5) then everything just works and the final result is correct: # widget3 = Media(js=['text-editor.js', 'text-editor-extras.js', 'color-picker.js']) widget3 = Media() widget3._js_lists = [['text-editor.js', 'text-editor-extras.js'], ['color-picker.js']] So what you proposed first (https://code.djangoproject.com/ticket/30179#comment:1) might just work fine and would be good enough (tm). Something like â€‹https://github.com/django/django/blob/543fc97407a932613d283c1e0bb47616cf8782e3/django/forms/widgets.py#L52 # Instead of self._js_lists = [js]: self._js_lists = list(js) if isinstance(js, set) else [js] @Matthias: I think that solution will work, but only if: 1) we're going to insist that users always use this notation wherever a \"non-dependency\" exists - i.e. it is considered user error for the user to forget to put color-picker.js in its own sub-list 2) we have a very tight definition of what a dependency is - e.g. color-picker.js can't legally be a dependency of text-editor.js / text-editor-extras.js, because it exists on its own in ColorPicker's media - which also invalidates the [jquery, widget1, noconflict] + [jquery, widget2, noconflict] case (does noconflict depend on widget1 or not?) I suspect you only have to go slightly before the complexity of [jquery, widget1, noconflict] + [jquery, widget2, noconflict] before you start running into counter-examples again. PR: â€‹https://github.com/django/django/pull/11010 I encountered another subtle bug along the way (which I suspect has existed since 1.x): #12879 calls for us to strip duplicates from the input lists, but in the current implementation the only de-duplication happens during Media.merge, so this never happens in the case of a single list. I've now extended the tests to cover this: â€‹https://github.com/django/django/pull/11010/files#diff-7fc04ae9019782c1884a0e97e96eda1eR154 . As a minor side effect of this extra de-duplication step, tuples get converted to lists more often, so I've had to fix up some existing tests accordingly - hopefully that's acceptable fall-out :-) Matt, great work. I believe it is best to merge all lists at once and not sequentially as I did. Based on your work, I would suggest to simply use the algorithms implemented in Python. Therefore the whole merge function can be replaced with a simple one liner: import heapq from collections import OrderedDict def merge(*sublists): return list(OrderedDict.fromkeys(heapq.merge(*sublists))) # >>> merge([3],[1],[1,2],[2,3]) # [1, 2, 3] It actually behaves different. I will continue to review your pull-request. As stated there, it would be helpful if there is some kind of resource to understand what strategy you implemented. For now I will try to review it without it."},"metadata":{"created_at":"2019-02-23T15:51:14Z","version":"3.0","environment_setup_commit":"419a78300f7cd27611196e1e464d50fd0385ff27"}}
{"task_id":"swebench_django_django_11039","source":"swebench-lite","input":{"repo":"django/django","instance_id":"django__django-11039","base_commit":"d5276398046ce4a102776a1e67dcac2884d80dfe","prompt":"sqlmigrate wraps it's outpout in BEGIN/COMMIT even if the database doesn't support transactional DDL Description (last modified by Simon Charette) The migration executor only adds the outer BEGIN/COMMIT â€‹if the migration is atomic and â€‹the schema editor can rollback DDL but the current sqlmigrate logic only takes migration.atomic into consideration. The issue can be addressed by Changing sqlmigrate â€‹assignment of self.output_transaction to consider connection.features.can_rollback_ddl as well. Adding a test in tests/migrations/test_commands.py based on â€‹an existing test for non-atomic migrations that mocks connection.features.can_rollback_ddl to False instead of overdidding MIGRATION_MODULES to point to a non-atomic migration. I marked the ticket as easy picking because I included the above guidelines but feel free to uncheck it if you deem it inappropriate.","hints_text":"I marked the ticket as easy picking because I included the above guidelines but feel free to uncheck it if you deem it inappropriate. Super. We don't have enough Easy Pickings tickets for the demand, so this kind of thing is great. (IMO ðŸ™‚) Hey, I'm working on this ticket, I would like you to know as this is my first ticket it may take little longer to complete :). Here is a â€‹| link to the working branch You may feel free to post references or elaborate more on the topic. Hi Parth. No problem. If you need help please reach out to e.g. â€‹django-core-mentorship citing this issue, and where you've got to/got stuck. Welcome aboard, and have fun! âœ¨"},"metadata":{"created_at":"2019-03-01T10:24:38Z","version":"3.0","environment_setup_commit":"419a78300f7cd27611196e1e464d50fd0385ff27"}}
{"task_id":"swebench_django_django_11049","source":"swebench-lite","input":{"repo":"django/django","instance_id":"django__django-11049","base_commit":"17455e924e243e7a55e8a38f45966d8cbb27c273","prompt":"Correct expected format in invalid DurationField error message Description If you enter a duration \"14:00\" into a duration field, it translates to \"00:14:00\" which is 14 minutes. The current error message for invalid DurationField says that this should be the format of durations: \"[DD] [HH:[MM:]]ss[.uuuuuu]\". But according to the actual behaviour, it should be: \"[DD] [[HH:]MM:]ss[.uuuuuu]\", because seconds are mandatory, minutes are optional, and hours are optional if minutes are provided. This seems to be a mistake in all Django versions that support the DurationField. Also the duration fields could have a default help_text with the requested format, because the syntax is not self-explanatory."},"metadata":{"created_at":"2019-03-03T09:56:16Z","version":"3.0","environment_setup_commit":"419a78300f7cd27611196e1e464d50fd0385ff27"}}
{"task_id":"swebench_matplotlib_matplotlib_18869","source":"swebench-lite","input":{"repo":"matplotlib/matplotlib","instance_id":"matplotlib__matplotlib-18869","base_commit":"b7d05919865fc0c37a0164cf467d5d5513bd0ede","prompt":"Add easily comparable version info to toplevel <!-- Welcome! Thanks for thinking of a way to improve Matplotlib. Before creating a new feature request please search the issues for relevant feature requests. --> ### Problem Currently matplotlib only exposes `__version__`. For quick version checks, exposing either a `version_info` tuple (which can be compared with other tuples) or a `LooseVersion` instance (which can be properly compared with other strings) would be a small usability improvement. (In practice I guess boring string comparisons will work just fine until we hit mpl 3.10 or 4.10 which is unlikely to happen soon, but that feels quite dirty :)) <!-- Provide a clear and concise description of the problem this feature will solve. For example: * I'm always frustrated when [...] because [...] * I would like it if [...] happened when I [...] because [...] * Here is a sample image of what I am asking for [...] --> ### Proposed Solution I guess I slightly prefer `LooseVersion`, but exposing just a `version_info` tuple is much more common in other packages (and perhaps simpler to understand). The hardest(?) part is probably just bikeshedding this point :-) <!-- Provide a clear and concise description of a way to accomplish what you want. For example: * Add an option so that when [...] [...] will happen --> ### Additional context and prior art `version_info` is a pretty common thing (citation needed). <!-- Add any other context or screenshots about the feature request here. You can also include links to examples of other programs that have something similar to your request. For example: * Another project [...] solved this by [...] -->","hints_text":"It seems that `__version_info__` is the way to go. ### Prior art - There's no official specification for version tuples. [PEP 396 - Module Version Numbers](https://www.python.org/dev/peps/pep-0396/) only defines the string `__version__`. - Many projects don't bother with version tuples. - When they do, `__version_info__` seems to be a common thing: - [Stackoverflow discussion](https://stackoverflow.com/a/466694) - [PySide2](https://doc.qt.io/qtforpython-5.12/pysideversion.html#printing-project-and-qt-version) uses it. - Python itself has the string [sys.version](https://docs.python.org/3/library/sys.html#sys.version) and the (named)tuple [sys.version_info](https://docs.python.org/3/library/sys.html#sys.version_info). In analogy to that `__version_info__` next to `__version__` makes sense for packages."},"metadata":{"created_at":"2020-11-01T23:18:42Z","version":"3.3","environment_setup_commit":"28289122be81e0bc0a6ee0c4c5b7343a46ce2e4e"}}
{"task_id":"swebench_matplotlib_matplotlib_22711","source":"swebench-lite","input":{"repo":"matplotlib/matplotlib","instance_id":"matplotlib__matplotlib-22711","base_commit":"f670fe78795b18eb1118707721852209cd77ad51","prompt":"[Bug]: cannot give init value for RangeSlider widget ### Bug summary I think `xy[4] = .25, val[0]` should be commented in /matplotlib/widgets. py\", line 915, in set_val as it prevents to initialized value for RangeSlider ### Code for reproduction ```python import numpy as np import matplotlib.pyplot as plt from matplotlib.widgets import RangeSlider # generate a fake image np.random.seed(19680801) N = 128 img = np.random.randn(N, N) fig, axs = plt.subplots(1, 2, figsize=(10, 5)) fig.subplots_adjust(bottom=0.25) im = axs[0].imshow(img) axs[1].hist(img.flatten(), bins='auto') axs[1].set_title('Histogram of pixel intensities') # Create the RangeSlider slider_ax = fig.add_axes([0.20, 0.1, 0.60, 0.03]) slider = RangeSlider(slider_ax, \"Threshold\", img.min(), img.max(),valinit=[0.0,0.0]) # Create the Vertical lines on the histogram lower_limit_line = axs[1].axvline(slider.val[0], color='k') upper_limit_line = axs[1].axvline(slider.val[1], color='k') def update(val): # The val passed to a callback by the RangeSlider will # be a tuple of (min, max) # Update the image's colormap im.norm.vmin = val[0] im.norm.vmax = val[1] # Update the position of the vertical lines lower_limit_line.set_xdata([val[0], val[0]]) upper_limit_line.set_xdata([val[1], val[1]]) # Redraw the figure to ensure it updates fig.canvas.draw_idle() slider.on_changed(update) plt.show() ``` ### Actual outcome ```python File \"<ipython-input-52-b704c53e18d4>\", line 19, in <module> slider = RangeSlider(slider_ax, \"Threshold\", img.min(), img.max(),valinit=[0.0,0.0]) File \"/Users/Vincent/opt/anaconda3/envs/py38/lib/python3.8/site-packages/matplotlib/widgets.py\", line 778, in __init__ self.set_val(valinit) File \"/Users/Vincent/opt/anaconda3/envs/py38/lib/python3.8/site-packages/matplotlib/widgets.py\", line 915, in set_val xy[4] = val[0], .25 IndexError: index 4 is out of bounds for axis 0 with size 4 ``` ### Expected outcome range slider with user initial values ### Additional information error can be removed by commenting this line ```python def set_val(self, val): \"\"\" Set slider value to *val*. Parameters ---------- val : tuple or array-like of float \"\"\" val = np.sort(np.asanyarray(val)) if val.shape != (2,): raise ValueError( f\"val must have shape (2,) but has shape {val.shape}\" ) val[0] = self._min_in_bounds(val[0]) val[1] = self._max_in_bounds(val[1]) xy = self.poly.xy if self.orientation == \"vertical\": xy[0] = .25, val[0] xy[1] = .25, val[1] xy[2] = .75, val[1] xy[3] = .75, val[0] # xy[4] = .25, val[0] else: xy[0] = val[0], .25 xy[1] = val[0], .75 xy[2] = val[1], .75 xy[3] = val[1], .25 # xy[4] = val[0], .25 self.poly.xy = xy self.valtext.set_text(self._format(val)) if self.drawon: self.ax.figure.canvas.draw_idle() self.val = val if self.eventson: self._observers.process(\"changed\", val) ``` ### Operating system OSX ### Matplotlib Version 3.5.1 ### Matplotlib Backend _No response_ ### Python version 3.8 ### Jupyter version _No response_ ### Installation pip","hints_text":"Huh, the polygon object must have changed inadvertently. Usually, you have to \"close\" the polygon by repeating the first vertex, but we make it possible for polygons to auto-close themselves. I wonder how (when?) this broke? On Tue, Mar 22, 2022 at 10:29 PM vpicouet ***@***.***> wrote: > Bug summary > > I think xy[4] = .25, val[0] should be commented in /matplotlib/widgets. > py\", line 915, in set_val > as it prevents to initialized value for RangeSlider > Code for reproduction > > import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.widgets import RangeSlider > # generate a fake imagenp.random.seed(19680801)N = 128img = np.random.randn(N, N) > fig, axs = plt.subplots(1, 2, figsize=(10, 5))fig.subplots_adjust(bottom=0.25) > im = axs[0].imshow(img)axs[1].hist(img.flatten(), bins='auto')axs[1].set_title('Histogram of pixel intensities') > # Create the RangeSliderslider_ax = fig.add_axes([0.20, 0.1, 0.60, 0.03])slider = RangeSlider(slider_ax, \"Threshold\", img.min(), img.max(),valinit=[0.0,0.0]) > # Create the Vertical lines on the histogramlower_limit_line = axs[1].axvline(slider.val[0], color='k')upper_limit_line = axs[1].axvline(slider.val[1], color='k') > > def update(val): > # The val passed to a callback by the RangeSlider will > # be a tuple of (min, max) > > # Update the image's colormap > im.norm.vmin = val[0] > im.norm.vmax = val[1] > > # Update the position of the vertical lines > lower_limit_line.set_xdata([val[0], val[0]]) > upper_limit_line.set_xdata([val[1], val[1]]) > > # Redraw the figure to ensure it updates > fig.canvas.draw_idle() > > slider.on_changed(update)plt.show() > > Actual outcome > > File \"<ipython-input-52-b704c53e18d4>\", line 19, in <module> > slider = RangeSlider(slider_ax, \"Threshold\", img.min(), img.max(),valinit=[0.0,0.0]) > > File \"/Users/Vincent/opt/anaconda3/envs/py38/lib/python3.8/site-packages/matplotlib/widgets.py\", line 778, in __init__ > self.set_val(valinit) > > File \"/Users/Vincent/opt/anaconda3/envs/py38/lib/python3.8/site-packages/matplotlib/widgets.py\", line 915, in set_val > xy[4] = val[0], .25 > > IndexError: index 4 is out of bounds for axis 0 with size 4 > > Expected outcome > > range slider with user initial values > Additional information > > error can be > > > def set_val(self, val): > \"\"\" > Set slider value to *val*. > > Parameters > ---------- > val : tuple or array-like of float > \"\"\" > val = np.sort(np.asanyarray(val)) > if val.shape != (2,): > raise ValueError( > f\"val must have shape (2,) but has shape {val.shape}\" > ) > val[0] = self._min_in_bounds(val[0]) > val[1] = self._max_in_bounds(val[1]) > xy = self.poly.xy > if self.orientation == \"vertical\": > xy[0] = .25, val[0] > xy[1] = .25, val[1] > xy[2] = .75, val[1] > xy[3] = .75, val[0] > # xy[4] = .25, val[0] > else: > xy[0] = val[0], .25 > xy[1] = val[0], .75 > xy[2] = val[1], .75 > xy[3] = val[1], .25 > # xy[4] = val[0], .25 > self.poly.xy = xy > self.valtext.set_text(self._format(val)) > if self.drawon: > self.ax.figure.canvas.draw_idle() > self.val = val > if self.eventson: > self._observers.process(\"changed\", val) > > > Operating system > > OSX > Matplotlib Version > > 3.5.1 > Matplotlib Backend > > *No response* > Python version > > 3.8 > Jupyter version > > *No response* > Installation > > pip > > â€” > Reply to this email directly, view it on GitHub > <https://github.com/matplotlib/matplotlib/issues/22686>, or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AACHF6CW2HVLKT5Q56BVZDLVBJ6X7ANCNFSM5RMUEIDQ> > . > You are receiving this because you are subscribed to this thread.Message > ID: ***@***.***> > Yes, i might have been too fast, cause it allows to skip the error but then it seems that the polygon is not right... Let me know if you know how this should be solved... ![Capture dâ€™eÌcran, le 2022-03-22 aÌ€ 23 20 23](https://user-images.githubusercontent.com/37241971/159617326-44c69bfc-bf0a-4f79-ab23-925c7066f2c2.jpg) So I think you found an edge case because your valinit has both values equal. This means that the poly object created by `axhspan` is not as large as the rest of the code expects. https://github.com/matplotlib/matplotlib/blob/11737d0694109f71d2603ba67d764aa2fb302761/lib/matplotlib/widgets.py#L722 A quick workaround is to have the valinit contain two different numbers (even if only minuscule difference) Yes you are right! Thanks a lot for digging into this!! Compare: ```python import matplotlib.pyplot as plt fig, ax = plt.subplots() poly_same_valinit = ax.axvspan(0, 0, 0, 1) poly_diff_valinit = ax.axvspan(0, .5, 0, 1) print(poly_same_valinit.xy.shape) print(poly_diff_valinit.xy.shape) ``` which gives: ``` (4, 2) (5, 2) ``` Two solutions spring to mind: 1. Easier option Throw an error in init if `valinit[0] == valinit[1]` 2. Maybe better option? Don't use axhspan and manually create the poly to ensure it always has the expected number of vertices Option 2 might be better yes @vpicouet any interest in opening a PR? I don't think I am qualified to do so, never opened a PR yet. RangeSlider might also contain another issue. When I call `RangeSlider.set_val([0.0,0.1])` It changes only the blue poly object and the range value on the right side of the slider not the dot: ![Capture dâ€™eÌcran, le 2022-03-25 aÌ€ 15 53 44](https://user-images.githubusercontent.com/37241971/160191943-aef5fbe2-2f54-42ae-9719-23375767b212.jpg) > I don't think I am qualified to do so, never opened a PR yet. That's always true until you've opened your first one :). But I also understand that it can be intimidating. > RangeSlider might also contain another issue. > When I call RangeSlider.set_val([0.0,0.1]) > It changes only the blue poly object and the range value on the right side of the slider not the dot: oh hmm - good catch! may be worth opening a separate issue there as these are two distinct bugs and this one may be a bit more comlicated to fix. Haha true! I might try when I have more time! Throwing an error at least as I have never worked with axhspan and polys. Ok, openning another issue. Can I try working on this? @ianhi @vpicouet From the discussion, I could identify that a quick fix would be to use a try-except block to throw an error if valinit[0] == valinit[1] Please let me know your thoughts. Sure!"},"metadata":{"created_at":"2022-03-27T00:34:37Z","version":"3.5","environment_setup_commit":"de98877e3dc45de8dd441d008f23d88738dc015d"}}
{"task_id":"swebench_matplotlib_matplotlib_22835","source":"swebench-lite","input":{"repo":"matplotlib/matplotlib","instance_id":"matplotlib__matplotlib-22835","base_commit":"c33557d120eefe3148ebfcf2e758ff2357966000","prompt":"[Bug]: scalar mappable format_cursor_data crashes on BoundarNorm ### Bug summary In 3.5.0 if you do: ```python import matplotlib.pyplot as plt import numpy as np import matplotlib as mpl fig, ax = plt.subplots() norm = mpl.colors.BoundaryNorm(np.linspace(-4, 4, 5), 256) X = np.random.randn(10, 10) pc = ax.imshow(X, cmap='RdBu_r', norm=norm) ``` and mouse over the image, it crashes with ``` File \"/Users/jklymak/matplotlib/lib/matplotlib/artist.py\", line 1282, in format_cursor_data neighbors = self.norm.inverse( File \"/Users/jklymak/matplotlib/lib/matplotlib/colors.py\", line 1829, in inverse raise ValueError(\"BoundaryNorm is not invertible\") ValueError: BoundaryNorm is not invertible ``` and interaction stops. Not sure if we should have a special check here, a try-except, or actually just make BoundaryNorm approximately invertible. ### Matplotlib Version main 3.5.0 [Bug]: scalar mappable format_cursor_data crashes on BoundarNorm ### Bug summary In 3.5.0 if you do: ```python import matplotlib.pyplot as plt import numpy as np import matplotlib as mpl fig, ax = plt.subplots() norm = mpl.colors.BoundaryNorm(np.linspace(-4, 4, 5), 256) X = np.random.randn(10, 10) pc = ax.imshow(X, cmap='RdBu_r', norm=norm) ``` and mouse over the image, it crashes with ``` File \"/Users/jklymak/matplotlib/lib/matplotlib/artist.py\", line 1282, in format_cursor_data neighbors = self.norm.inverse( File \"/Users/jklymak/matplotlib/lib/matplotlib/colors.py\", line 1829, in inverse raise ValueError(\"BoundaryNorm is not invertible\") ValueError: BoundaryNorm is not invertible ``` and interaction stops. Not sure if we should have a special check here, a try-except, or actually just make BoundaryNorm approximately invertible. ### Matplotlib Version main 3.5.0","hints_text":"I think the correct fix is to specifically check for BoundaryNorm there and implement special logic to determine the positions of the neighboring intervals (on the BoundaryNorm) for that case. I tried returning the passed in `value` instead of the exception at https://github.com/matplotlib/matplotlib/blob/b2bb7be4ba343fcec6b1dbbffd7106e6af240221/lib/matplotlib/colors.py#L1829 ```python return value ``` and it seems to work. At least the error is gone and the values displayed in the plot (bottom right) when hovering with the mouse seem also right. But the numbers are rounded to only 1 digit in sci notation. So 19, 20 or 21 become 2.e+01. Hope this helps. Maybe a more constructive suggestion for a change in https://github.com/matplotlib/matplotlib/blob/b2bb7be4ba343fcec6b1dbbffd7106e6af240221/lib/matplotlib/artist.py#L1280 that tries to fix the error: ```python ends = self.norm(np.array([self.norm.vmin, self.norm.vmax])) if np.isfinite(normed) and np.allclose(ends, 0.5, rtol=0.0, atol=0.5): ``` This way, because `BoundaryNorm` doesn't map to 0...1 range but to the indices of the colors, the call to `BoundaryNorm.inverse()` is skipped and the default value for `g_sig_digits=3` is used. But I'm not sure if there can be a case where `BoundaryNorm` actually maps its endpoints to 0...1, in which case all breaks down. hey, I just ran into this while plotting data... (interactivity doesn't stop but it plots a lot of issues) any updates? @raphaelquast no, this still needs someone to work on it. Labeled as a good first issue as there in no API design here (just returning an empty string is better than the current state). I can give a PR, that is based on my [last comment](https://github.com/matplotlib/matplotlib/issues/21915#issuecomment-992454625) a try. But my concerns/questions from then are still valid. As this is all internal code, I think we should just do an `isinstance(norm, BoundaryNorm)` check and then act accordingly. There is already a bunch of instances of this in the colorbar code as precedence. After thinking about my suggestion again, I now understand why you prefer an `isinstace` check. (In my initial suggestion one would have to check if `ends` maps to (0,1) and if `normed` is in [0, 1] for the correct way to implement my idea. But these conditions are taylored to catch `BoundaryNorm` anyway, so why not do it directly.) However, I suggest using a try block after https://github.com/matplotlib/matplotlib/blob/c33557d120eefe3148ebfcf2e758ff2357966000/lib/matplotlib/artist.py#L1305 ```python # Midpoints of neighboring color intervals. try: neighbors = self.norm.inverse( (int(normed * n) + np.array([0, 1])) / n) except ValueError: # non-invertible ScalarMappable ``` because `BoundaryNorm` raises a `ValueError` anyway and I assume this to be the case for all non-invertible `ScalarMappables`. (And the issue is the exception raised from `inverse`). The question is: What to put in the `except`? So what is \"acting accordingly\" in this case? If `BoundaryNorm` isn't invertible, how can we reliably get the data values of the midpoints of neighboring colors? I think it should be enough to get the boundaries themselves, instead of the midpoints (though both is possible) with something like: ```python cur_idx = np.argmin(np.abs(self.norm.boundaries - data)) cur_bound = self.norm.boundaries[cur_idx] neigh_idx = cur_idx + 1 if data > cur_bound else cur_idx - 1 neighbors = self.norm.boundaries[ np.array([cur_idx, neigh_idx]) ] ``` Problems with this code are: - `boundaries` property is specific to `BoundaryNorm`, isn't it? So we gained nothing by using `try .. except` - more checks are needed to cover all cases for `clip` and `extend` options of `BoundaryNorm` such that `neigh_idx` is always in bounds - for very coarse boundaries compared to data (ie: `boundaries = [0,1,2,3,4,5]; data = random(n)*5) the displayed values are always rounded to one significant digit. While in principle, this is expected (I believe), it results in inconsistency. In my example 0.111111 would be given as 0.1 and 0.001 as 0.001 and 1.234 would be just 1. > boundaries property is specific to BoundaryNorm, isn't it? So we gained nothing by using try .. except This is one argument in favor of doing the `isinstance` check because then you can assert we _have_ a BoundaryNorm and can trust that you can access its state etc. One on hand, duck typeing is in general a Good Thing in Python and we should err on the side of being forgiving on input, but sometimes the complexity of it is more trouble than it is worth if you really only have 1 case that you are trying catch! > I assume this to be the case for all non-invertible ScalarMappables. However we only (at this point) are talking about a way to recover in the case of BoundaryNorm. Let everything else continue to fail and we will deal with those issues if/when they get reported. I think the correct fix is to specifically check for BoundaryNorm there and implement special logic to determine the positions of the neighboring intervals (on the BoundaryNorm) for that case. I tried returning the passed in `value` instead of the exception at https://github.com/matplotlib/matplotlib/blob/b2bb7be4ba343fcec6b1dbbffd7106e6af240221/lib/matplotlib/colors.py#L1829 ```python return value ``` and it seems to work. At least the error is gone and the values displayed in the plot (bottom right) when hovering with the mouse seem also right. But the numbers are rounded to only 1 digit in sci notation. So 19, 20 or 21 become 2.e+01. Hope this helps. Maybe a more constructive suggestion for a change in https://github.com/matplotlib/matplotlib/blob/b2bb7be4ba343fcec6b1dbbffd7106e6af240221/lib/matplotlib/artist.py#L1280 that tries to fix the error: ```python ends = self.norm(np.array([self.norm.vmin, self.norm.vmax])) if np.isfinite(normed) and np.allclose(ends, 0.5, rtol=0.0, atol=0.5): ``` This way, because `BoundaryNorm` doesn't map to 0...1 range but to the indices of the colors, the call to `BoundaryNorm.inverse()` is skipped and the default value for `g_sig_digits=3` is used. But I'm not sure if there can be a case where `BoundaryNorm` actually maps its endpoints to 0...1, in which case all breaks down. hey, I just ran into this while plotting data... (interactivity doesn't stop but it plots a lot of issues) any updates? @raphaelquast no, this still needs someone to work on it. Labeled as a good first issue as there in no API design here (just returning an empty string is better than the current state). I can give a PR, that is based on my [last comment](https://github.com/matplotlib/matplotlib/issues/21915#issuecomment-992454625) a try. But my concerns/questions from then are still valid. As this is all internal code, I think we should just do an `isinstance(norm, BoundaryNorm)` check and then act accordingly. There is already a bunch of instances of this in the colorbar code as precedence. After thinking about my suggestion again, I now understand why you prefer an `isinstace` check. (In my initial suggestion one would have to check if `ends` maps to (0,1) and if `normed` is in [0, 1] for the correct way to implement my idea. But these conditions are taylored to catch `BoundaryNorm` anyway, so why not do it directly.) However, I suggest using a try block after https://github.com/matplotlib/matplotlib/blob/c33557d120eefe3148ebfcf2e758ff2357966000/lib/matplotlib/artist.py#L1305 ```python # Midpoints of neighboring color intervals. try: neighbors = self.norm.inverse( (int(normed * n) + np.array([0, 1])) / n) except ValueError: # non-invertible ScalarMappable ``` because `BoundaryNorm` raises a `ValueError` anyway and I assume this to be the case for all non-invertible `ScalarMappables`. (And the issue is the exception raised from `inverse`). The question is: What to put in the `except`? So what is \"acting accordingly\" in this case? If `BoundaryNorm` isn't invertible, how can we reliably get the data values of the midpoints of neighboring colors? I think it should be enough to get the boundaries themselves, instead of the midpoints (though both is possible) with something like: ```python cur_idx = np.argmin(np.abs(self.norm.boundaries - data)) cur_bound = self.norm.boundaries[cur_idx] neigh_idx = cur_idx + 1 if data > cur_bound else cur_idx - 1 neighbors = self.norm.boundaries[ np.array([cur_idx, neigh_idx]) ] ``` Problems with this code are: - `boundaries` property is specific to `BoundaryNorm`, isn't it? So we gained nothing by using `try .. except` - more checks are needed to cover all cases for `clip` and `extend` options of `BoundaryNorm` such that `neigh_idx` is always in bounds - for very coarse boundaries compared to data (ie: `boundaries = [0,1,2,3,4,5]; data = random(n)*5) the displayed values are always rounded to one significant digit. While in principle, this is expected (I believe), it results in inconsistency. In my example 0.111111 would be given as 0.1 and 0.001 as 0.001 and 1.234 would be just 1. > boundaries property is specific to BoundaryNorm, isn't it? So we gained nothing by using try .. except This is one argument in favor of doing the `isinstance` check because then you can assert we _have_ a BoundaryNorm and can trust that you can access its state etc. One on hand, duck typeing is in general a Good Thing in Python and we should err on the side of being forgiving on input, but sometimes the complexity of it is more trouble than it is worth if you really only have 1 case that you are trying catch! > I assume this to be the case for all non-invertible ScalarMappables. However we only (at this point) are talking about a way to recover in the case of BoundaryNorm. Let everything else continue to fail and we will deal with those issues if/when they get reported."},"metadata":{"created_at":"2022-04-12T23:13:58Z","version":"3.5","environment_setup_commit":"de98877e3dc45de8dd441d008f23d88738dc015d"}}
{"task_id":"swebench_matplotlib_matplotlib_23299","source":"swebench-lite","input":{"repo":"matplotlib/matplotlib","instance_id":"matplotlib__matplotlib-23299","base_commit":"3eadeacc06c9f2ddcdac6ae39819faa9fbee9e39","prompt":"[Bug]: get_backend() clears figures from Gcf.figs if they were created under rc_context ### Bug summary calling `matplotlib.get_backend()` removes all figures from `Gcf` if the *first* figure in `Gcf.figs` was created in an `rc_context`. ### Code for reproduction ```python import matplotlib.pyplot as plt from matplotlib import get_backend, rc_context # fig1 = plt.figure() # <- UNCOMMENT THIS LINE AND IT WILL WORK # plt.ion() # <- ALTERNATIVELY, UNCOMMENT THIS LINE AND IT WILL ALSO WORK with rc_context(): fig2 = plt.figure() before = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}' get_backend() after = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}' assert before == after, '\\n' + before + '\\n' + after ``` ### Actual outcome ``` --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-1-fa4d099aa289> in <cell line: 11>() 9 after = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}' 10 ---> 11 assert before == after, '\\n' + before + '\\n' + after 12 AssertionError: 94453354309744 OrderedDict([(1, <matplotlib.backends.backend_qt.FigureManagerQT object at 0x7fb33e26c220>)]) 94453354309744 OrderedDict() ``` ### Expected outcome The figure should not be missing from `Gcf`. Consequences of this are, e.g, `plt.close(fig2)` doesn't work because `Gcf.destroy_fig()` can't find it. ### Additional information _No response_ ### Operating system Xubuntu ### Matplotlib Version 3.5.2 ### Matplotlib Backend QtAgg ### Python version Python 3.10.4 ### Jupyter version n/a ### Installation conda","hints_text":"My knee-jerk guess is that : - the `rcParams['backend']` in the auto-sentinel - that is stashed by rc_context - if you do the first thing to force the backend to be resolved in the context manager it get changes - the context manager sets it back to the sentinel an the way out - `get_backend()` re-resolves the backend which because it changes the backend it closes all of the figures This is probably been a long standing latent bug, but was brought to the front when we made the backend resolution lazier."},"metadata":{"created_at":"2022-06-18T01:34:39Z","version":"3.5","environment_setup_commit":"de98877e3dc45de8dd441d008f23d88738dc015d"}}
{"task_id":"swebench_matplotlib_matplotlib_23314","source":"swebench-lite","input":{"repo":"matplotlib/matplotlib","instance_id":"matplotlib__matplotlib-23314","base_commit":"97fc1154992f64cfb2f86321155a7404efeb2d8a","prompt":"[Bug]: set_visible() not working for 3d projection ### Bug summary in the subplot projection=\"3d\" the set_visible function doesn't work even if the value is set to False ### Code for reproduction ```python import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={'projection': '3d'}) ax1.scatter(1,1,1) ax2.scatter(1,1,1, c='r') ax1.set_visible(False) plt.show() # Thanks Tim for your help! ``` ### Actual outcome the subplot remains visible which should not happen if the value is set to False ### Expected outcome the subplot is not visible if the value is set to False ### Additional information _No response_ ### Operating system _No response_ ### Matplotlib Version 3.4.2 ### Matplotlib Backend Qt5Agg ### Python version 3.8.10 ### Jupyter version _No response_ ### Installation _No response_","hints_text":"Please try to boil down the problem to a minimal example when reporting issues. I've now done that for you: ``` import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={'projection': '3d'}) ax1.scatter(1,1,1) ax2.scatter(1,1,1, c='r') ax1.set_visible(False) plt.show() ``` **Output** ![grafik](https://user-images.githubusercontent.com/2836374/174673179-f5b14df5-7689-49eb-995a-4c97e31c3c43.png) **Expected** The left axes should be invisible."},"metadata":{"created_at":"2022-06-21T02:41:34Z","version":"3.5","environment_setup_commit":"de98877e3dc45de8dd441d008f23d88738dc015d"}}
{"task_id":"swebench_matplotlib_matplotlib_23476","source":"swebench-lite","input":{"repo":"matplotlib/matplotlib","instance_id":"matplotlib__matplotlib-23476","base_commit":"33a0599711d26dc2b79f851c6daed4947df7c167","prompt":"[Bug]: DPI of a figure is doubled after unpickling on M1 Mac ### Bug summary When a figure is unpickled, it's dpi is doubled. This behaviour happens every time and if done in a loop it can cause an `OverflowError`. ### Code for reproduction ```python import numpy as np import matplotlib import matplotlib.pyplot as plt import pickle import platform print(matplotlib.get_backend()) print('Matplotlib ver:', matplotlib.__version__) print('Platform:', platform.platform()) print('System:', platform.system()) print('Release:', platform.release()) print('Python ver:', platform.python_version()) def dump_load_get_dpi(fig): with open('sinus.pickle','wb') as file: pickle.dump(fig, file) with open('sinus.pickle', 'rb') as blob: fig2 = pickle.load(blob) return fig2, fig2.dpi def run(): fig = plt.figure() x = np.linspace(0,2*np.pi) y = np.sin(x) for i in range(32): print(f'{i}: {fig.dpi}') fig, dpi = dump_load_get_dpi(fig) if __name__ == '__main__': run() ``` ### Actual outcome ``` MacOSX Matplotlib ver: 3.5.2 Platform: macOS-12.4-arm64-arm-64bit System: Darwin Release: 21.5.0 Python ver: 3.9.12 0: 200.0 1: 400.0 2: 800.0 3: 1600.0 4: 3200.0 5: 6400.0 6: 12800.0 7: 25600.0 8: 51200.0 9: 102400.0 10: 204800.0 11: 409600.0 12: 819200.0 13: 1638400.0 14: 3276800.0 15: 6553600.0 16: 13107200.0 17: 26214400.0 18: 52428800.0 19: 104857600.0 20: 209715200.0 21: 419430400.0 Traceback (most recent call last): File \"/Users/wsykala/projects/matplotlib/example.py\", line 34, in <module> run() File \"/Users/wsykala/projects/matplotlib/example.py\", line 30, in run fig, dpi = dump_load_get_dpi(fig) File \"/Users/wsykala/projects/matplotlib/example.py\", line 20, in dump_load_get_dpi fig2 = pickle.load(blob) File \"/Users/wsykala/miniconda3/envs/playground/lib/python3.9/site-packages/matplotlib/figure.py\", line 2911, in __setstate__ mgr = plt._backend_mod.new_figure_manager_given_figure(num, self) File \"/Users/wsykala/miniconda3/envs/playground/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 3499, in new_figure_manager_given_figure canvas = cls.FigureCanvas(figure) File \"/Users/wsykala/miniconda3/envs/playground/lib/python3.9/site-packages/matplotlib/backends/backend_macosx.py\", line 32, in __init__ _macosx.FigureCanvas.__init__(self, width, height) OverflowError: signed integer is greater than maximum ``` ### Expected outcome ``` MacOSX Matplotlib ver: 3.5.2 Platform: macOS-12.4-arm64-arm-64bit System: Darwin Release: 21.5.0 Python ver: 3.9.12 0: 200.0 1: 200.0 2: 200.0 3: 200.0 4: 200.0 5: 200.0 6: 200.0 7: 200.0 8: 200.0 9: 200.0 10: 200.0 11: 200.0 12: 200.0 13: 200.0 14: 200.0 15: 200.0 16: 200.0 17: 200.0 18: 200.0 19: 200.0 20: 200.0 21: 200.0 22: 200.0 ``` ### Additional information This seems to happen only on M1 MacBooks and the version of python doesn't matter. ### Operating system OS/X ### Matplotlib Version 3.5.2 ### Matplotlib Backend MacOSX ### Python version 3.9.12 ### Jupyter version _No response_ ### Installation pip","hints_text":"I suspect this will also affect anything that know how to deal with high-dpi screens. For, .... reasons..., when we handle high-dpi cases by doubling the dpi on the figure (we have ideas how to fix this, but it is a fair amount of work) when we show it. We are saving the doubled dpi which when re-loaded in doubled again. I think there is an easy fix....."},"metadata":{"created_at":"2022-07-22T18:58:22Z","version":"3.5","environment_setup_commit":"de98877e3dc45de8dd441d008f23d88738dc015d"}}
{"task_id":"swebench_mwaskom_seaborn_2848","source":"swebench-lite","input":{"repo":"mwaskom/seaborn","instance_id":"mwaskom__seaborn-2848","base_commit":"94621cef29f80282436d73e8d2c0aa76dab81273","prompt":"pairplot fails with hue_order not containing all hue values in seaborn 0.11.1 In seaborn < 0.11, one could plot only a subset of the values in the hue column, by passing a hue_order list containing only the desired values. Points with hue values not in the list were simply not plotted. ```python iris = sns.load_dataset(\"iris\")` # The hue column contains three different species; here we want to plot two sns.pairplot(iris, hue=\"species\", hue_order=[\"setosa\", \"versicolor\"]) ``` This no longer works in 0.11.1. Passing a hue_order list that does not contain some of the values in the hue column raises a long, ugly error traceback. The first exception arises in seaborn/_core.py: ``` TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' ``` seaborn version: 0.11.1 matplotlib version: 3.3.2 matplotlib backends: MacOSX, Agg or jupyter notebook inline.","hints_text":"The following workarounds seem to work: ``` g.map(sns.scatterplot, hue=iris[\"species\"], hue_order=iris[\"species\"].unique()) ``` or ``` g.map(lambda x, y, **kwargs: sns.scatterplot(x=x, y=y, hue=iris[\"species\"])) ``` > ``` > g.map(sns.scatterplot, hue=iris[\"species\"], hue_order=iris[\"species\"].unique()) > ``` The workaround fixes the problem for me. Thank you very much! @mwaskom Should I close the Issue or leave it open until the bug is fixed? That's a good workaround, but it's still a bug. The problem is that `PairGrid` now lets `hue` at the grid-level delegate to the axes-level functions if they have `hue` in their signature. But it's not properly handling the case where `hue` is *not* set for the grid, but *is* specified for one mapped function. @jhncls's workaround suggests the fix. An easier workaround would have been to set `PairGrid(..., hue=\"species\")` and then pass `.map(..., hue=None)` where you don't want to separate by species. But `regplot` is the one axis-level function that does not yet handle hue-mapping internally, so it doesn't work for this specific case. It would have if you wanted a single bivariate density over hue-mapped scatterplot points (i.e. [this example](http://seaborn.pydata.org/introduction.html#classes-and-functions-for-making-complex-graphics) or something similar."},"metadata":{"created_at":"2022-06-11T18:21:32Z","version":"0.12","environment_setup_commit":"d25872b0fc99dbf7e666a91f59bd4ed125186aa1"}}
{"task_id":"swebench_mwaskom_seaborn_3010","source":"swebench-lite","input":{"repo":"mwaskom/seaborn","instance_id":"mwaskom__seaborn-3010","base_commit":"0f5a013e2cf43562deec3b879458e59a73853813","prompt":"PolyFit is not robust to missing data ```python so.Plot([1, 2, 3, None, 4], [1, 2, 3, 4, 5]).add(so.Line(), so.PolyFit()) ``` <details><summary>Traceback</summary> ```python-traceback --------------------------------------------------------------------------- LinAlgError Traceback (most recent call last) File ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/IPython/core/formatters.py:343, in BaseFormatter.__call__(self, obj) 341 method = get_real_method(obj, self.print_method) 342 if method is not None: --> 343 return method() 344 return None 345 else: File ~/code/seaborn/seaborn/_core/plot.py:265, in Plot._repr_png_(self) 263 def _repr_png_(self) -> tuple[bytes, dict[str, float]]: --> 265 return self.plot()._repr_png_() File ~/code/seaborn/seaborn/_core/plot.py:804, in Plot.plot(self, pyplot) 800 \"\"\" 801 Compile the plot spec and return the Plotter object. 802 \"\"\" 803 with theme_context(self._theme_with_defaults()): --> 804 return self._plot(pyplot) File ~/code/seaborn/seaborn/_core/plot.py:822, in Plot._plot(self, pyplot) 819 plotter._setup_scales(self, common, layers, coord_vars) 821 # Apply statistical transform(s) --> 822 plotter._compute_stats(self, layers) 824 # Process scale spec for semantic variables and coordinates computed by stat 825 plotter._setup_scales(self, common, layers) File ~/code/seaborn/seaborn/_core/plot.py:1110, in Plotter._compute_stats(self, spec, layers) 1108 grouper = grouping_vars 1109 groupby = GroupBy(grouper) -> 1110 res = stat(df, groupby, orient, scales) 1112 if pair_vars: 1113 data.frames[coord_vars] = res File ~/code/seaborn/seaborn/_stats/regression.py:41, in PolyFit.__call__(self, data, groupby, orient, scales) 39 def __call__(self, data, groupby, orient, scales): ---> 41 return groupby.apply(data, self._fit_predict) File ~/code/seaborn/seaborn/_core/groupby.py:109, in GroupBy.apply(self, data, func, *args, **kwargs) 106 grouper, groups = self._get_groups(data) 108 if not grouper: --> 109 return self._reorder_columns(func(data, *args, **kwargs), data) 111 parts = {} 112 for key, part_df in data.groupby(grouper, sort=False): File ~/code/seaborn/seaborn/_stats/regression.py:30, in PolyFit._fit_predict(self, data) 28 xx = yy = [] 29 else: ---> 30 p = np.polyfit(x, y, self.order) 31 xx = np.linspace(x.min(), x.max(), self.gridsize) 32 yy = np.polyval(p, xx) File <__array_function__ internals>:180, in polyfit(*args, **kwargs) File ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/numpy/lib/polynomial.py:668, in polyfit(x, y, deg, rcond, full, w, cov) 666 scale = NX.sqrt((lhs*lhs).sum(axis=0)) 667 lhs /= scale --> 668 c, resids, rank, s = lstsq(lhs, rhs, rcond) 669 c = (c.T/scale).T # broadcast scale coefficients 671 # warn on rank reduction, which indicates an ill conditioned matrix File <__array_function__ internals>:180, in lstsq(*args, **kwargs) File ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/numpy/linalg/linalg.py:2300, in lstsq(a, b, rcond) 2297 if n_rhs == 0: 2298 # lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis 2299 b = zeros(b.shape[:-2] + (m, n_rhs + 1), dtype=b.dtype) -> 2300 x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj) 2301 if m == 0: 2302 x[...] = 0 File ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/numpy/linalg/linalg.py:101, in _raise_linalgerror_lstsq(err, flag) 100 def _raise_linalgerror_lstsq(err, flag): --> 101 raise LinAlgError(\"SVD did not converge in Linear Least Squares\") LinAlgError: SVD did not converge in Linear Least Squares ``` </details>"},"metadata":{"created_at":"2022-09-11T19:37:32Z","version":"0.12","environment_setup_commit":"d25872b0fc99dbf7e666a91f59bd4ed125186aa1"}}
{"task_id":"swebench_mwaskom_seaborn_3190","source":"swebench-lite","input":{"repo":"mwaskom/seaborn","instance_id":"mwaskom__seaborn-3190","base_commit":"4a9e54962a29c12a8b103d75f838e0e795a6974d","prompt":"Color mapping fails with boolean data ```python so.Plot([\"a\", \"b\"], [1, 2], color=[True, False]).add(so.Bar()) ``` ```python-traceback --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ... File ~/code/seaborn/seaborn/_core/plot.py:841, in Plot._plot(self, pyplot) 838 plotter._compute_stats(self, layers) 840 # Process scale spec for semantic variables and coordinates computed by stat --> 841 plotter._setup_scales(self, common, layers) 843 # TODO Remove these after updating other methods 844 # ---- Maybe have debug= param that attaches these when True? 845 plotter._data = common File ~/code/seaborn/seaborn/_core/plot.py:1252, in Plotter._setup_scales(self, p, common, layers, variables) 1250 self._scales[var] = Scale._identity() 1251 else: -> 1252 self._scales[var] = scale._setup(var_df[var], prop) 1254 # Everything below here applies only to coordinate variables 1255 # We additionally skip it when we're working with a value 1256 # that is derived from a coordinate we've already processed. 1257 # e.g., the Stat consumed y and added ymin/ymax. In that case, 1258 # we've already setup the y scale and ymin/max are in scale space. 1259 if axis is None or (var != coord and coord in p._variables): File ~/code/seaborn/seaborn/_core/scales.py:351, in ContinuousBase._setup(self, data, prop, axis) 349 vmin, vmax = axis.convert_units((vmin, vmax)) 350 a = forward(vmin) --> 351 b = forward(vmax) - forward(vmin) 353 def normalize(x): 354 return (x - a) / b TypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead. ```","hints_text":"Would this simply mean refactoring the code to use `^` or `xor` functions instead?"},"metadata":{"created_at":"2022-12-18T17:13:51Z","version":"0.12","environment_setup_commit":"d25872b0fc99dbf7e666a91f59bd4ed125186aa1"}}
{"task_id":"swebench_mwaskom_seaborn_3407","source":"swebench-lite","input":{"repo":"mwaskom/seaborn","instance_id":"mwaskom__seaborn-3407","base_commit":"515286e02be3e4c0ff2ef4addb34a53c4a676ee4","prompt":"pairplot raises KeyError with MultiIndex DataFrame When trying to pairplot a MultiIndex DataFrame, `pairplot` raises a `KeyError`: MRE: ```python import numpy as np import pandas as pd import seaborn as sns data = { (\"A\", \"1\"): np.random.rand(100), (\"A\", \"2\"): np.random.rand(100), (\"B\", \"1\"): np.random.rand(100), (\"B\", \"2\"): np.random.rand(100), } df = pd.DataFrame(data) sns.pairplot(df) ``` Output: ``` [c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/seaborn/axisgrid.py) in pairplot(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, markers, height, aspect, corner, dropna, plot_kws, diag_kws, grid_kws, size) 2142 diag_kws.setdefault(\"legend\", False) 2143 if diag_kind == \"hist\": -> 2144 grid.map_diag(histplot, **diag_kws) 2145 elif diag_kind == \"kde\": 2146 diag_kws.setdefault(\"fill\", True) [c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/seaborn/axisgrid.py) in map_diag(self, func, **kwargs) 1488 plt.sca(ax) 1489 -> 1490 vector = self.data[var] 1491 if self._hue_var is not None: 1492 hue = self.data[self._hue_var] [c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/pandas/core/frame.py) in __getitem__(self, key) 3765 if is_iterator(key): 3766 key = list(key) -> 3767 indexer = self.columns._get_indexer_strict(key, \"columns\")[1] 3768 3769 # take() does not accept boolean indexers [c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/pandas/core/indexes/multi.py) in _get_indexer_strict(self, key, axis_name) 2534 indexer = self._get_indexer_level_0(keyarr) 2535 -> 2536 self._raise_if_missing(key, indexer, axis_name) 2537 return self[indexer], indexer 2538 [c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/pandas/core/indexes/multi.py) in _raise_if_missing(self, key, indexer, axis_name) 2552 cmask = check == -1 2553 if cmask.any(): -> 2554 raise KeyError(f\"{keyarr[cmask]} not in index\") 2555 # We get here when levels still contain values which are not 2556 # actually in Index anymore KeyError: \"['1'] not in index\" ``` A workaround is to \"flatten\" the columns: ```python df.columns = [\"\".join(column) for column in df.columns] ```"},"metadata":{"created_at":"2023-06-27T23:17:29Z","version":"0.13","environment_setup_commit":"23860365816440b050e9211e1c395a966de3c403"}}
{"task_id":"swebench_pallets_flask_4045","source":"swebench-lite","input":{"repo":"pallets/flask","instance_id":"pallets__flask-4045","base_commit":"d8c37f43724cd9fb0870f77877b7c4c7e38a19e0","prompt":"Raise error when blueprint name contains a dot This is required since every dot is now significant since blueprints can be nested. An error was already added for endpoint names in 1.0, but should have been added for this as well."},"metadata":{"created_at":"2021-05-13T21:32:41Z","version":"2.0","environment_setup_commit":"4346498c85848c53843b810537b83a8f6124c9d3"}}
{"task_id":"swebench_pallets_flask_4992","source":"swebench-lite","input":{"repo":"pallets/flask","instance_id":"pallets__flask-4992","base_commit":"4c288bc97ea371817199908d0d9b12de9dae327e","prompt":"Add a file mode parameter to flask.Config.from_file() Python 3.11 introduced native TOML support with the `tomllib` package. This could work nicely with the `flask.Config.from_file()` method as an easy way to load TOML config files: ```python app.config.from_file(\"config.toml\", tomllib.load) ``` However, `tomllib.load()` takes an object readable in binary mode, while `flask.Config.from_file()` opens a file in text mode, resulting in this error: ``` TypeError: File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')` ``` We can get around this with a more verbose expression, like loading from a file opened with the built-in `open()` function and passing the `dict` to `app.Config.from_mapping()`: ```python # We have to repeat the path joining that from_file() does with open(os.path.join(app.config.root_path, \"config.toml\"), \"rb\") as file: app.config.from_mapping(tomllib.load(file)) ``` But adding a file mode parameter to `flask.Config.from_file()` would enable the use of a simpler expression. E.g.: ```python app.config.from_file(\"config.toml\", tomllib.load, mode=\"b\") ```","hints_text":"You can also use: ```python app.config.from_file(\"config.toml\", lambda f: tomllib.load(f.buffer)) ``` Thanks - I was looking for another way to do it. I'm happy with that for now, although it's worth noting this about `io.TextIOBase.buffer` from the docs: > This is not part of the [TextIOBase](https://docs.python.org/3/library/io.html#io.TextIOBase) API and may not exist in some implementations. Oh, didn't mean for you to close this, that was just a shorter workaround. I think a `text=True` parameter would be better, easier to use `True` or `False` rather than mode strings. Some libraries, like `tomllib`, have _Opinions_ about whether text or bytes are correct for parsing files, and we can accommodate that. can i work on this? No need to ask to work on an issue. As long as the issue is not assigned to anyone and doesn't have have a linked open PR (both can be seen in the sidebar), anyone is welcome to work on any issue."},"metadata":{"created_at":"2023-02-22T14:00:17Z","version":"2.3","environment_setup_commit":"182ce3dd15dfa3537391c3efaf9c3ff407d134d4"}}
{"task_id":"swebench_pallets_flask_5063","source":"swebench-lite","input":{"repo":"pallets/flask","instance_id":"pallets__flask-5063","base_commit":"182ce3dd15dfa3537391c3efaf9c3ff407d134d4","prompt":"Flask routes to return domain/sub-domains information Currently when checking **flask routes** it provides all routes but **it is no way to see which routes are assigned to which subdomain**. **Default server name:** SERVER_NAME: 'test.local' **Domains (sub-domains):** test.test.local admin.test.local test.local **Adding blueprints:** app.register_blueprint(admin_blueprint,url_prefix='',subdomain='admin') app.register_blueprint(test_subdomain_blueprint,url_prefix='',subdomain='test') ``` $ flask routes * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them. Endpoint Methods Rule ------------------------------------------------------- --------- ------------------------------------------------ admin_blueprint.home GET /home test_subdomain_blueprint.home GET /home static GET /static/<path:filename> ... ``` **Feature request** It will be good to see something like below (that will make more clear which route for which subdomain, because now need to go and check configuration). **If it is not possible to fix routes**, can you add or tell which method(s) should be used to get below information from flask? ``` $ flask routes * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them. Domain Endpoint Methods Rule ----------------- ---------------------------------------------------- ---------- ------------------------------------------------ admin.test.local admin_blueprint.home GET /home test.test.local test_subdomain_blueprint.home GET /home test.local static GET /static/<path:filename> ... ```"},"metadata":{"created_at":"2023-04-14T16:36:54Z","version":"2.3","environment_setup_commit":"182ce3dd15dfa3537391c3efaf9c3ff407d134d4"}}
{"task_id":"swebench_psf_requests_1963","source":"swebench-lite","input":{"repo":"psf/requests","instance_id":"psf__requests-1963","base_commit":"110048f9837f8441ea536804115e80b69f400277","prompt":"`Session.resolve_redirects` copies the original request for all subsequent requests, can cause incorrect method selection Consider the following redirection chain: ``` POST /do_something HTTP/1.1 Host: server.example.com ... HTTP/1.1 303 See Other Location: /new_thing_1513 GET /new_thing_1513 Host: server.example.com ... HTTP/1.1 307 Temporary Redirect Location: //failover.example.com/new_thing_1513 ``` The intermediate 303 See Other has caused the POST to be converted to a GET. The subsequent 307 should preserve the GET. However, because `Session.resolve_redirects` starts each iteration by copying the _original_ request object, Requests will issue a POST!","hints_text":"Uh, yes, that's a bug. =D This is also a good example of something that there's no good way to write a test for with httpbin as-is. This can be tested though, without httpbin, and I'll tackle this one tonight or this weekend. I've tinkered with `resolve_redirects` enough to be certain enough that I caused this. As such I feel its my responsibility to fix it."},"metadata":{"created_at":"2014-03-15T17:42:11Z","version":"2.3","environment_setup_commit":"3eb69be879063de4803f7f0152b83738a1c95ca4"}}
{"task_id":"swebench_psf_requests_2148","source":"swebench-lite","input":{"repo":"psf/requests","instance_id":"psf__requests-2148","base_commit":"fe693c492242ae532211e0c173324f09ca8cf227","prompt":"socket.error exception not caught/wrapped in a requests exception (ConnectionError perhaps?) I just noticed a case where I had a socket reset on me, and was raised to me as a raw socket error as opposed to something like a requests.exceptions.ConnectionError: ``` File \"/home/rtdean/***/***/***/***/***/***.py\", line 67, in dir_parse root = ElementTree.fromstring(response.text) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/models.py\", line 721, in text if not self.content: File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/models.py\", line 694, in content self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes() File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/models.py\", line 627, in generate for chunk in self.raw.stream(chunk_size, decode_content=True): File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/packages/urllib3/response.py\", line 240, in stream data = self.read(amt=amt, decode_content=decode_content) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/packages/urllib3/response.py\", line 187, in read data = self._fp.read(amt) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/httplib.py\", line 543, in read return self._read_chunked(amt) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/httplib.py\", line 612, in _read_chunked value.append(self._safe_read(chunk_left)) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/httplib.py\", line 658, in _safe_read chunk = self.fp.read(min(amt, MAXAMOUNT)) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/socket.py\", line 380, in read data = self._sock.recv(left) File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/gevent-1.0.1-py2.7-linux-x86_64.egg/gevent/socket.py\", line 385, in recv return sock.recv(*args) socket.error: [Errno 104] Connection reset by peer ``` Not sure if this is by accident or design... in general, I guess I'd expect a requests exception when using requests, but I can start looking for socket errors and the like as well.","hints_text":"No, this looks like an error. `iter_content` doesn't seem to expect any socket errors, but it should. We need to fix this."},"metadata":{"created_at":"2014-07-24T21:03:03Z","version":"2.3","environment_setup_commit":"3eb69be879063de4803f7f0152b83738a1c95ca4"}}
{"task_id":"swebench_psf_requests_2317","source":"swebench-lite","input":{"repo":"psf/requests","instance_id":"psf__requests-2317","base_commit":"091991be0da19de9108dbe5e3752917fea3d7fdc","prompt":"method = builtin_str(method) problem In requests/sessions.py is a command: method = builtin_str(method) Converts method from bâ€™GETâ€™ to \"b'GETâ€™\" Which is the literal string, no longer a binary string. When requests tries to use the method \"b'GETâ€™â€, it gets a 404 Not Found response. I am using python3.4 and python-neutronclient (2.3.9) with requests (2.4.3). neutronclient is broken because it uses this \"args = utils.safe_encode_list(args)\" command which converts all the values to binary string, including method. I'm not sure if this is a bug with neutronclient or a bug with requests, but I'm starting here. Seems if requests handled the method value being a binary string, we wouldn't have any problem. Also, I tried in python2.6 and this bug doesn't exist there. Some difference between 2.6 and 3.4 makes this not work right.","hints_text":"Ugh. This should have been caught and replaced with `to_native_str`. This is definitely a requests bug."},"metadata":{"created_at":"2014-11-01T02:20:16Z","version":"2.4","environment_setup_commit":"091991be0da19de9108dbe5e3752917fea3d7fdc"}}
{"task_id":"swebench_psf_requests_2674","source":"swebench-lite","input":{"repo":"psf/requests","instance_id":"psf__requests-2674","base_commit":"0be38a0c37c59c4b66ce908731da15b401655113","prompt":"urllib3 exceptions passing through requests API I don't know if it's a design goal of requests to hide urllib3's exceptions and wrap them around requests.exceptions types. (If it's not IMHO it should be, but that's another discussion) If it is, I have at least two of them passing through that I have to catch in addition to requests' exceptions. They are requests.packages.urllib3.exceptions.DecodeError and requests.packages.urllib3.exceptions.TimeoutError (this one I get when a proxy timeouts) Thanks!","hints_text":"I definitely agree with you and would agree that these should be wrapped. Could you give us stack-traces so we can find where they're bleeding through? Sorry I don't have stack traces readily available :/ No worries. I have ideas as to where the DecodeError might be coming from but I'm not certain where the TimeoutError could be coming from. If you run into them again, please save us the stack traces. =) Thanks for reporting them. (We'll never know what we're missing until someone tells us.) `TimeoutError` is almost certainly being raised from either [`HTTPConnectionPool.urlopen()`](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L282-L293) or from [`HTTPConnection.putrequest()`](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L301). Adding a new clause to [here](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L323-L335) should cover us. Actually, that can't be right, we should be catching and rethrowing as a Requests `Timeout` exception in that block. Hmm, I'll do another spin through the code to see if I can see the problem. Yeah, a quick search of the `urllib3` code reveals that the only place that `TimeoutError`s are thrown is from `HTTPConnectionPool.urlopen()`. These should not be leaking. We really need a stack trace to track this down. I've added a few logs to get the traces if they happen again. What may have confused me for the TimeoutError is that requests' Timeout actually wraps the urllib3's TimeoutError and we were logging the content of the error as well. So DecodeError was definitely being thrown but probably not TimeoutError, sorry for the confusion. I'll report here it I ever see it happening now that we're watching for it. Thanks for the help! I also got urllib3 exceptions passing through when use Session in several threads, trace: ``` ...... File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 347, in get return self.request('GET', url, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 335, in request resp = self.send(prep, **send_kwargs) File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests\\adapters.py\", line 292, in send timeout=timeout File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 423, in url open conn = self._get_conn(timeout=pool_timeout) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 224, in _ge t_conn raise ClosedPoolError(self, \"Pool is closed.\") ClosedPoolError: HTTPConnectionPool(host='......', port=80): Pool is closed. ``` Ah, we should rewrap that `ClosedPoolError` too. But it's still the summer... How can any pool be closed? :smirk_cat: But yes :+1: I've added a fix for the `ClosedPoolError` to #1475. Which apparently broke in the last month for no adequately understandable reason. If it's still needed, here is the traceback of DecodeError I got using proxy on requests 2.0.0: ``` Traceback (most recent call last): File \"/home/krat/Projects/Grubhub/source/Pit/pit/web.py\", line 52, in request response = session.request(method, url, **kw) File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/sessions.py\", line 357, in request resp = self.send(prep, **send_kwargs) File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/adapters.py\", line 367, in send r.content File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/models.py\", line 633, in content self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes() File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/models.py\", line 572, in generate decode_content=True): File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 225, in stream data = self.read(amt=amt, decode_content=decode_content) File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 193, in read e) DecodeError: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing: incorrect header check',)) ``` Slightly different to the above, but urllib3's LocationParseError leaks through which could probably do with being wrapped in InvalidURL. ``` Traceback (most recent call last): File \"/home/oliver/wc/trunk/mtmCore/python/asagent/samplers/net/web.py\", line 255, in process_url resp = self.request(self.params.httpverb, url, data=data) File \"/home/oliver/wc/trunk/mtmCore/python/asagent/samplers/net/web.py\", line 320, in request verb, url, data=data)) File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/sessions.py\", line 286, in prepare_request File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/models.py\", line 286, in prepare File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/models.py\", line 333, in prepare_url File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/packages/urllib3/util.py\", line 397, in parse_url LocationParseError: Failed to parse: Failed to parse: fe80::5054:ff:fe5a:fc0 ```"},"metadata":{"created_at":"2015-07-17T08:33:52Z","version":"2.7","environment_setup_commit":"bf436ea0a49513bd4e49bb2d1645bd770e470d75"}}
{"task_id":"swebench_psf_requests_3362","source":"swebench-lite","input":{"repo":"psf/requests","instance_id":"psf__requests-3362","base_commit":"36453b95b13079296776d11b09cab2567ea3e703","prompt":"Uncertain about content/text vs iter_content(decode_unicode=True/False) When requesting an application/json document, I'm seeing `next(r.iter_content(16*1024, decode_unicode=True))` returning bytes, whereas `r.text` returns unicode. My understanding was that both should return a unicode object. In essence, I thought \"iter_content\" was equivalent to \"iter_text\" when decode_unicode was True. Have I misunderstood something? I can provide an example if needed. For reference, I'm using python 3.5.1 and requests 2.10.0. Thanks!","hints_text":"what does (your response object).encoding return? There's at least one key difference: `decode_unicode=True` doesn't fall back to `apparent_encoding`, which means it'll never autodetect the encoding. This means if `response.encoding` is None it is a no-op: in fact, it's a no-op that yields bytes. That behaviour seems genuinely bad to me, so I think we should consider it a bug. I'd rather we had the same logic as in `text` for this. `r.encoding` returns `None`. On a related note, `iter_text` might be clearer/more consistent than `iter_content(decode_unicode=True)` if there's room for change in the APIs future (and `iter_content_lines` and `iter_text_lines` I guess), assuming you don't see that as bloat. @mikepelley The API is presently frozen so I don't think we'll be adding those three methods. Besides, `iter_text` likely wouldn't provide much extra value outside of calling `iter_content(decode_unicode=True)`."},"metadata":{"created_at":"2016-06-24T13:31:31Z","version":"2.10","environment_setup_commit":"36453b95b13079296776d11b09cab2567ea3e703"}}
{"task_id":"swebench_psf_requests_863","source":"swebench-lite","input":{"repo":"psf/requests","instance_id":"psf__requests-863","base_commit":"a0df2cbb10419037d11d04352b3175405ab52941","prompt":"Allow lists in the dict values of the hooks argument Currently the Request class has a .register_hook() method but it parses the dictionary it expects from it's hooks argument weirdly: the argument can only specify one hook function per hook. If you pass in a list of hook functions per hook the code in Request.**init**() will wrap the list in a list which then fails when the hooks are consumed (since a list is not callable). This is especially annoying since you can not use multiple hooks from a session. The only way to get multiple hooks now is to create the request object without sending it, then call .register_hook() multiple times and then finally call .send(). This would all be much easier if Request.**init**() parsed the hooks parameter in a way that it accepts lists as it's values.","hints_text":"If anyone OKs this feature request, I'd be happy to dig into it. @sigmavirus24 :+1: Just need to make sure that the current workflow also continues to work with this change. Once @kennethreitz has time to review #833, I'll start working on this. I have a feeling opening a branch for this would cause a merge conflict if I were to have two Pull Requests that are ignorant of each other for the same file. Could be wrong though. Also, I'm in no rush since I'm fairly busy and I know @kennethreitz is more busy than I am with conferences and whatnot. Just wanted to keep @flub updated. I'm going to start work on this Friday at the earliest."},"metadata":{"created_at":"2012-09-20T15:48:00Z","version":"0.14","environment_setup_commit":"a0df2cbb10419037d11d04352b3175405ab52941"}}
{"task_id":"swebench_pydata_xarray_3364","source":"swebench-lite","input":{"repo":"pydata/xarray","instance_id":"pydata__xarray-3364","base_commit":"863e49066ca4d61c9adfe62aca3bf21b90e1af8c","prompt":"Ignore missing variables when concatenating datasets? Several users (@raj-kesavan, @richardotis, now myself) have wondered about how to concatenate xray Datasets with different variables. With the current `xray.concat`, you need to awkwardly create dummy variables filled with `NaN` in datasets that don't have them (or drop mismatched variables entirely). Neither of these are great options -- `concat` should have an option (the default?) to take care of this for the user. This would also be more consistent with `pd.concat`, which takes a more relaxed approach to matching dataframes with different variables (it does an outer join).","hints_text":"Closing as stale, please reopen if still relevant"},"metadata":{"created_at":"2019-10-01T21:15:54Z","version":"0.12","environment_setup_commit":"1c198a191127c601d091213c4b3292a8bb3054e1"}}
{"task_id":"swebench_pydata_xarray_4094","source":"swebench-lite","input":{"repo":"pydata/xarray","instance_id":"pydata__xarray-4094","base_commit":"a64cf2d5476e7bbda099b34c40b7be1880dbd39a","prompt":"to_unstacked_dataset broken for single-dim variables <!-- A short summary of the issue, if appropriate --> #### MCVE Code Sample ```python arr = xr.DataArray( np.arange(3), coords=[(\"x\", [0, 1, 2])], ) data = xr.Dataset({\"a\": arr, \"b\": arr}) stacked = data.to_stacked_array('y', sample_dims=['x']) unstacked = stacked.to_unstacked_dataset('y') # MergeError: conflicting values for variable 'y' on objects to be combined. You can skip this check by specifying compat='override'. ``` #### Expected Output A working roundtrip. #### Problem Description I need to stack a bunch of variables and later unstack them again, however this doesn't work if the variables only have a single dimension. #### Versions <details><summary>Output of <tt>xr.show_versions()</tt></summary> INSTALLED VERSIONS ------------------ commit: None python: 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0] python-bits: 64 OS: Linux OS-release: 4.15.0-96-generic machine: x86_64 processor: x86_64 byteorder: little LC_ALL: None LANG: en_GB.UTF-8 LOCALE: en_GB.UTF-8 libhdf5: 1.10.4 libnetcdf: 4.6.2 xarray: 0.15.1 pandas: 1.0.3 numpy: 1.17.3 scipy: 1.3.1 netCDF4: 1.4.2 pydap: None h5netcdf: None h5py: 2.10.0 Nio: None zarr: None cftime: 1.0.4.2 nc_time_axis: None PseudoNetCDF: None rasterio: None cfgrib: None iris: None bottleneck: None dask: 2.10.1 distributed: 2.10.0 matplotlib: 3.1.1 cartopy: None seaborn: 0.10.0 numbagg: None setuptools: 41.0.0 pip: 19.0.3 conda: 4.8.3 pytest: 5.3.5 IPython: 7.9.0 sphinx: None </details>"},"metadata":{"created_at":"2020-05-26T00:36:02Z","version":"0.12","environment_setup_commit":"1c198a191127c601d091213c4b3292a8bb3054e1"}}
{"task_id":"swebench_pydata_xarray_4248","source":"swebench-lite","input":{"repo":"pydata/xarray","instance_id":"pydata__xarray-4248","base_commit":"98dc1f4ea18738492e074e9e51ddfed5cd30ab94","prompt":"Feature request: show units in dataset overview Here's a hypothetical dataset: ``` <xarray.Dataset> Dimensions: (time: 3, x: 988, y: 822) Coordinates: * x (x) float64 ... * y (y) float64 ... * time (time) datetime64[ns] ... Data variables: rainfall (time, y, x) float32 ... max_temp (time, y, x) float32 ... ``` It would be really nice if the units of the coordinates and of the data variables were shown in the `Dataset` repr, for example as: ``` <xarray.Dataset> Dimensions: (time: 3, x: 988, y: 822) Coordinates: * x, in metres (x) float64 ... * y, in metres (y) float64 ... * time (time) datetime64[ns] ... Data variables: rainfall, in mm (time, y, x) float32 ... max_temp, in deg C (time, y, x) float32 ... ```","hints_text":"I would love to see this. What would we want the exact formatting to be? Square brackets to copy how units from `attrs['units']` are displayed on plots? e.g. ``` <xarray.Dataset> Dimensions: (time: 3, x: 988, y: 822) Coordinates: * x [m] (x) float64 ... * y [m] (y) float64 ... * time [s] (time) datetime64[ns] ... Data variables: rainfall [mm] (time, y, x) float32 ... max_temp [deg C] (time, y, x) float32 ... ``` The lack of vertical alignment is kind of ugly... There are now two cases to discuss: units in `attrs`, and unit-aware arrays like pint. (If we do the latter we may not need the former though...) from @keewis on #3616: >At the moment, the formatting.diff_*_repr functions that provide the pretty-printing for assert_* use repr to format NEP-18 strings, truncating the result if it is too long. In the case of pint's quantities, this makes the pretty printing useless since only a few values are visible and the unit is in the truncated part. > > What should we about this? Does pint have to change its repr? We could presumably just extract the units from pint's repr to display them separately. I don't know if that raises questions about generality of duck-typing arrays though @dcherian ? Is it fine to make units a special-case? it was argued in pint that the unit is part of the data, so we should keep it as close to the data as possible. How about ``` <xarray.Dataset> Dimensions: (time: 3, x: 988, y: 822) Coordinates: * x (x) [m] float64 ... * y (y) [m] float64 ... * time (time) [s] datetime64[ns] ... Data variables: rainfall (time, y, x) [mm] float32 ... max_temp (time, y, x) [deg C] float32 ... ``` or ``` <xarray.Dataset> Dimensions: (time: 3, x: 988, y: 822) Coordinates: * x (x) float64 [m] ... * y (y) float64 [m] ... * time (time) datetime64[ns] [s] ... Data variables: rainfall (time, y, x) float32 [mm] ... max_temp (time, y, x) float32 [deg C] ... ``` The issue with the second example is that it is easy to confuse with numpy's dtype, though. Maybe we should use parentheses instead? re special casing: I think would be fine for attributes since we already special case them for plotting, but I don't know about duck arrays. Even if we want to special case them, there are many unit libraries with different interfaces so we would either need to special case all of them or require a specific interface (or a function to retrieve the necessary data?). Also, we should keep in mind is that using more horizontal space for the units results in less space for data. And we should not forget about https://github.com/dask/dask/issues/5329#issue-485927396, where a different kind of format was proposed, at least for the values of a `DataArray`. Instead of trying to come up with our own formatting, how about supporting a `_repr_short_(self, length)` method on the duck array (with a fall back to the current behavior)? That way duck arrays have to explicitly define the format (or have a compatibility package like `pint-xarray` provide it for them) if they want something different from their normal repr and we don't have to add duck array specific code. This won't help with displaying the `units` attributes (which we don't really need once we have support for pint arrays in indexes)."},"metadata":{"created_at":"2020-07-22T14:54:03Z","version":"0.12","environment_setup_commit":"1c198a191127c601d091213c4b3292a8bb3054e1"}}
{"task_id":"swebench_pydata_xarray_4493","source":"swebench-lite","input":{"repo":"pydata/xarray","instance_id":"pydata__xarray-4493","base_commit":"a5f53e203c52a7605d5db799864046471115d04f","prompt":"DataSet.update causes chunked dask DataArray to evalute its values eagerly **What happened**: Used `DataSet.update` to update a chunked dask DataArray, but the DataArray is no longer chunked after the update. **What you expected to happen**: The chunked DataArray should still be chunked after the update **Minimal Complete Verifiable Example**: ```python foo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk() # foo is chunked ds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])}) # foo is still chunked here ds # you can verify that foo is chunked ``` ```python update_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])} update_dict[\"foo\"][1] # foo is still chunked ``` ```python ds.update(update_dict) ds # now foo is no longer chunked ``` **Environment**: <details><summary>Output of <tt>xr.show_versions()</tt></summary> ``` commit: None python: 3.8.3 (default, Jul 2 2020, 11:26:31) [Clang 10.0.0 ] python-bits: 64 OS: Darwin OS-release: 19.6.0 machine: x86_64 processor: i386 byteorder: little LC_ALL: None LANG: en_US.UTF-8 LOCALE: en_US.UTF-8 libhdf5: 1.10.6 libnetcdf: None xarray: 0.16.0 pandas: 1.0.5 numpy: 1.18.5 scipy: 1.5.0 netCDF4: None pydap: None h5netcdf: None h5py: 2.10.0 Nio: None zarr: None cftime: None nc_time_axis: None PseudoNetCDF: None rasterio: None cfgrib: None iris: None bottleneck: None dask: 2.20.0 distributed: 2.20.0 matplotlib: 3.2.2 cartopy: None seaborn: None numbagg: None pint: None setuptools: 49.2.0.post20200714 pip: 20.1.1 conda: None pytest: 5.4.3 IPython: 7.16.1 sphinx: None ``` </details> Dataset constructor with DataArray triggers computation Is it intentional that creating a Dataset with a DataArray and dimension names for a single variable causes computation of that variable? In other words, why does ```xr.Dataset(dict(a=('d0', xr.DataArray(da.random.random(10)))))``` cause the dask array to compute? A longer example: ```python import dask.array as da import xarray as xr x = da.random.randint(1, 10, size=(100, 25)) ds = xr.Dataset(dict(a=xr.DataArray(x, dims=('x', 'y')))) type(ds.a.data) dask.array.core.Array # Recreate the dataset with the same array, but also redefine the dimensions ds2 = xr.Dataset(dict(a=(('x', 'y'), ds.a)) type(ds2.a.data) numpy.ndarray ```","hints_text":"that's because `xarray.core.variable.as_compatible_data` doesn't consider `DataArray` objects: https://github.com/pydata/xarray/blob/333e8dba55f0165ccadf18f2aaaee9257a4d716b/xarray/core/variable.py#L202-L203 and thus falls back to `DataArray.values`: https://github.com/pydata/xarray/blob/333e8dba55f0165ccadf18f2aaaee9257a4d716b/xarray/core/variable.py#L219 I think that's a bug and it should be fine to use ```python if isinstance(data, (DataArray, Variable)): return data.data ``` but I didn't check if that would break anything. Are you up for sending in a PR? For now, you can work around that by manually retrieving `data`: ```python In [2]: foo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk() # foo is chunked ...: ds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])}) # foo is still chunked here ...: ds Out[2]: <xarray.Dataset> Dimensions: (x: 3, y: 3) Dimensions without coordinates: x, y Data variables: foo (x, y) float64 dask.array<chunksize=(3, 3), meta=np.ndarray> bar (x) int64 1 2 3 In [3]: ds2 = ds.assign( ...: { ...: \"foo\": lambda ds: ((\"x\", \"y\"), ds.foo[1:, :].data), ...: \"bar\": lambda ds: (\"x\", ds.bar[1:]), ...: } ...: ) ...: ds2 Out[3]: <xarray.Dataset> Dimensions: (x: 2, y: 3) Dimensions without coordinates: x, y Data variables: foo (x, y) float64 dask.array<chunksize=(2, 3), meta=np.ndarray> bar (x) int64 2 3 ``` > xarray.core.variable.as_compatible_data doesn't consider DataArray objects: I don't think DataArrays are expected at that level though. BUT I'm probably wrong. > {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])} This syntax is weird. You should be able to do `update_dict = {\"foo\": ds.foo[1:, :], \"bar\": ds.bar[1:]}` . For the simple example, `ds.update(update_dict)` and `ds.assign(update_dict)` both fail because you can't align dimensions without labels when the dimension size is different between variables (I find this confusing). @chunhochow What are you trying to do? Overwrite the existing `foo` and `bar` variables? > when the dimension size is different between variables (I find this confusing). I guess the issue is that the dataset has `x` at a certain size and by reassigning we're trying to set `x` to a different size. I *think* the failure is expected in this case, and it could be solved by assigning labels to `x`. Thinking about the initial problem some more, it might be better to simply point to `isel`: ```python ds2 = ds.isel(x=slice(1, None)) ds2 ``` should do the same, but without having to worry about manually reconstructing a valid dataset. Yes, I'm trying to drop the last \"bin\" of data (the edge has problems) along all the DataArrays along the dimension `x`, But I couldn't figure out the syntax for how to do it from reading the documentation. Thank you! I will try `isel` next week when I get back to it!"},"metadata":{"created_at":"2020-10-06T22:00:41Z","version":"0.12","environment_setup_commit":"1c198a191127c601d091213c4b3292a8bb3054e1"}}
{"task_id":"swebench_pydata_xarray_5131","source":"swebench-lite","input":{"repo":"pydata/xarray","instance_id":"pydata__xarray-5131","base_commit":"e56905889c836c736152b11a7e6117a229715975","prompt":"Trailing whitespace in DatasetGroupBy text representation When displaying a DatasetGroupBy in an interactive Python session, the first line of output contains a trailing whitespace. The first example in the documentation demonstrate this: ```pycon >>> import xarray as xr, numpy as np >>> ds = xr.Dataset( ... {\"foo\": ((\"x\", \"y\"), np.random.rand(4, 3))}, ... coords={\"x\": [10, 20, 30, 40], \"letters\": (\"x\", list(\"abba\"))}, ... ) >>> ds.groupby(\"letters\") DatasetGroupBy, grouped over 'letters' 2 groups with labels 'a', 'b'. ``` There is a trailing whitespace in the first line of output which is \"DatasetGroupBy, grouped over 'letters' \". This can be seen more clearly by converting the object to a string (note the whitespace before `\\n`): ```pycon >>> str(ds.groupby(\"letters\")) \"DatasetGroupBy, grouped over 'letters' \\n2 groups with labels 'a', 'b'.\" ``` While this isn't a problem in itself, it causes an issue for us because we use flake8 in continuous integration to verify that our code is correctly formatted and we also have doctests that rely on DatasetGroupBy textual representation. Flake8 reports a violation on the trailing whitespaces in our docstrings. If we remove the trailing whitespaces, our doctests fail because the expected output doesn't match the actual output. So we have conflicting constraints coming from our tools which both seem reasonable. Trailing whitespaces are forbidden by flake8 because, among other reasons, they lead to noisy git diffs. Doctest want the expected output to be exactly the same as the actual output and considers a trailing whitespace to be a significant difference. We could configure flake8 to ignore this particular violation for the files in which we have these doctests, but this may cause other trailing whitespaces to creep in our code, which we don't want. Unfortunately it's not possible to just add `# NoQA` comments to get flake8 to ignore the violation only for specific lines because that creates a difference between expected and actual output from doctest point of view. Flake8 doesn't allow to disable checks for blocks of code either. Is there a reason for having this trailing whitespace in DatasetGroupBy representation? Whould it be OK to remove it? If so please let me know and I can make a pull request.","hints_text":"I don't think this is intentional and we are happy to take a PR. The problem seems to be here: https://github.com/pydata/xarray/blob/c7c4aae1fa2bcb9417e498e7dcb4acc0792c402d/xarray/core/groupby.py#L439 You will also have to fix the tests (maybe other places): https://github.com/pydata/xarray/blob/c7c4aae1fa2bcb9417e498e7dcb4acc0792c402d/xarray/tests/test_groupby.py#L391 https://github.com/pydata/xarray/blob/c7c4aae1fa2bcb9417e498e7dcb4acc0792c402d/xarray/tests/test_groupby.py#L408"},"metadata":{"created_at":"2021-04-08T09:19:30Z","version":"0.12","environment_setup_commit":"1c198a191127c601d091213c4b3292a8bb3054e1"}}
{"task_id":"swebench_pylint_dev_pylint_5859","source":"swebench-lite","input":{"repo":"pylint-dev/pylint","instance_id":"pylint-dev__pylint-5859","base_commit":"182cc539b8154c0710fcea7e522267e42eba8899","prompt":"\"--notes\" option ignores note tags that are entirely punctuation ### Bug description If a note tag specified with the `--notes` option is entirely punctuation, pylint won't report a fixme warning (W0511). ```python # YES: yes # ???: no ``` `pylint test.py --notes=\"YES,???\"` will return a fixme warning (W0511) for the first line, but not the second. ### Configuration ```ini Default ``` ### Command used ```shell pylint test.py --notes=\"YES,???\" ``` ### Pylint output ```shell ************* Module test test.py:1:1: W0511: YES: yes (fixme) ``` ### Expected behavior ``` ************* Module test test.py:1:1: W0511: YES: yes (fixme) test.py:2:1: W0511: ???: no (fixme) ``` ### Pylint version ```shell pylint 2.12.2 astroid 2.9.0 Python 3.10.2 (main, Feb 2 2022, 05:51:25) [Clang 13.0.0 (clang-1300.0.29.3)] ``` ### OS / Environment macOS 11.6.1 ### Additional dependencies _No response_","hints_text":"Did a little investigation, this is we're actually converting this option in a regular expression pattern (thereby making it awfully similar to the `notes-rgx` option). Since `?` is a special character in regex this doesn't get picked up. Using `\\?\\?\\?` in either `notes` or `notes-rgx` should work."},"metadata":{"created_at":"2022-03-04T00:01:54Z","version":"2.13","environment_setup_commit":"3b2fbaec045697d53bdd4435e59dbfc2b286df4b"}}
{"task_id":"swebench_pylint_dev_pylint_6506","source":"swebench-lite","input":{"repo":"pylint-dev/pylint","instance_id":"pylint-dev__pylint-6506","base_commit":"0a4204fd7555cfedd43f43017c94d24ef48244a5","prompt":"Traceback printed for unrecognized option ### Bug description A traceback is printed when an unrecognized option is passed to pylint. ### Configuration _No response_ ### Command used ```shell pylint -Q ``` ### Pylint output ```shell ************* Module Command line Command line:1:0: E0015: Unrecognized option found: Q (unrecognized-option) Traceback (most recent call last): File \"/Users/markbyrne/venv310/bin/pylint\", line 33, in <module> sys.exit(load_entry_point('pylint', 'console_scripts', 'pylint')()) File \"/Users/markbyrne/programming/pylint/pylint/__init__.py\", line 24, in run_pylint PylintRun(argv or sys.argv[1:]) File \"/Users/markbyrne/programming/pylint/pylint/lint/run.py\", line 135, in __init__ args = _config_initialization( File \"/Users/markbyrne/programming/pylint/pylint/config/config_initialization.py\", line 85, in _config_initialization raise _UnrecognizedOptionError(options=unrecognized_options) pylint.config.exceptions._UnrecognizedOptionError ``` ### Expected behavior The top part of the current output is handy: `Command line:1:0: E0015: Unrecognized option found: Q (unrecognized-option)` The traceback I don't think is expected & not user-friendly. A usage tip, for example: ```python mypy -Q usage: mypy [-h] [-v] [-V] [more options; see below] [-m MODULE] [-p PACKAGE] [-c PROGRAM_TEXT] [files ...] mypy: error: unrecognized arguments: -Q ``` ### Pylint version ```shell pylint 2.14.0-dev0 astroid 2.11.3 Python 3.10.0b2 (v3.10.0b2:317314165a, May 31 2021, 10:02:22) [Clang 12.0.5 (clang-1205.0.22.9)] ``` ### OS / Environment _No response_ ### Additional dependencies _No response_","hints_text":"@Pierre-Sassoulas Agreed that this is a blocker for `2.14` but not necessarily for the beta. This is just a \"nice-to-have\". Thanks @mbyrnepr2 for reporting though! ðŸ‘ the blocker are for the final release only. We could add a 'beta-blocker' label, that would be very humorous !"},"metadata":{"created_at":"2022-05-05T13:01:41Z","version":"2.14","environment_setup_commit":"680edebc686cad664bbed934a490aeafa775f163"}}
{"task_id":"swebench_pylint_dev_pylint_7080","source":"swebench-lite","input":{"repo":"pylint-dev/pylint","instance_id":"pylint-dev__pylint-7080","base_commit":"3c5eca2ded3dd2b59ebaf23eb289453b5d2930f0","prompt":"`--recursive=y` ignores `ignore-paths` ### Bug description When running recursively, it seems `ignore-paths` in my settings in pyproject.toml is completely ignored ### Configuration ```ini [tool.pylint.MASTER] ignore-paths = [ # Auto generated \"^src/gen/.*$\", ] ``` ### Command used ```shell pylint --recursive=y src/ ``` ### Pylint output ```shell ************* Module region_selection src\\region_selection.py:170:0: R0914: Too many local variables (17/15) (too-many-locals) ************* Module about src\\gen\\about.py:2:0: R2044: Line with empty comment (empty-comment) src\\gen\\about.py:4:0: R2044: Line with empty comment (empty-comment) src\\gen\\about.py:57:0: C0301: Line too long (504/120) (line-too-long) src\\gen\\about.py:12:0: C0103: Class name \"Ui_AboutAutoSplitWidget\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name) src\\gen\\about.py:12:0: R0205: Class 'Ui_AboutAutoSplitWidget' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance) src\\gen\\about.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\about.py:13:22: C0103: Argument name \"AboutAutoSplitWidget\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\about.py:53:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\about.py:53:28: C0103: Argument name \"AboutAutoSplitWidget\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\about.py:24:8: W0201: Attribute 'ok_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\about.py:27:8: W0201: Attribute 'created_by_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\about.py:30:8: W0201: Attribute 'version_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\about.py:33:8: W0201: Attribute 'donate_text_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\about.py:37:8: W0201: Attribute 'donate_button_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\about.py:43:8: W0201: Attribute 'icon_label' defined outside __init__ (attribute-defined-outside-init) ************* Module design src\\gen\\design.py:2:0: R2044: Line with empty comment (empty-comment) src\\gen\\design.py:4:0: R2044: Line with empty comment (empty-comment) src\\gen\\design.py:328:0: C0301: Line too long (123/120) (line-too-long) src\\gen\\design.py:363:0: C0301: Line too long (125/120) (line-too-long) src\\gen\\design.py:373:0: C0301: Line too long (121/120) (line-too-long) src\\gen\\design.py:412:0: C0301: Line too long (131/120) (line-too-long) src\\gen\\design.py:12:0: C0103: Class name \"Ui_MainWindow\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name) src\\gen\\design.py:308:8: C0103: Attribute name \"actionSplit_Settings\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:318:8: C0103: Attribute name \"actionCheck_for_Updates_on_Open\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:323:8: C0103: Attribute name \"actionLoop_Last_Split_Image_To_First_Image\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:325:8: C0103: Attribute name \"actionAuto_Start_On_Reset\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:327:8: C0103: Attribute name \"actionGroup_dummy_splits_when_undoing_skipping\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:12:0: R0205: Class 'Ui_MainWindow' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance) src\\gen\\design.py:12:0: R0902: Too many instance attributes (69/15) (too-many-instance-attributes) src\\gen\\design.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:13:22: C0103: Argument name \"MainWindow\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:16:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:13:4: R0915: Too many statements (339/50) (too-many-statements) src\\gen\\design.py:354:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:354:28: C0103: Argument name \"MainWindow\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\design.py:354:4: R0915: Too many statements (61/50) (too-many-statements) src\\gen\\design.py:31:8: W0201: Attribute 'central_widget' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:33:8: W0201: Attribute 'x_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:36:8: W0201: Attribute 'select_region_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:40:8: W0201: Attribute 'start_auto_splitter_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:44:8: W0201: Attribute 'reset_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:49:8: W0201: Attribute 'undo_split_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:54:8: W0201: Attribute 'skip_split_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:59:8: W0201: Attribute 'check_fps_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:63:8: W0201: Attribute 'fps_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:66:8: W0201: Attribute 'live_image' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:75:8: W0201: Attribute 'current_split_image' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:81:8: W0201: Attribute 'current_image_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:85:8: W0201: Attribute 'width_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:88:8: W0201: Attribute 'height_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:91:8: W0201: Attribute 'fps_value_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:95:8: W0201: Attribute 'width_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:101:8: W0201: Attribute 'height_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:107:8: W0201: Attribute 'capture_region_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:111:8: W0201: Attribute 'current_image_file_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:115:8: W0201: Attribute 'take_screenshot_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:119:8: W0201: Attribute 'x_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:128:8: W0201: Attribute 'y_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:136:8: W0201: Attribute 'y_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:139:8: W0201: Attribute 'align_region_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:143:8: W0201: Attribute 'select_window_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:147:8: W0201: Attribute 'browse_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:151:8: W0201: Attribute 'split_image_folder_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:154:8: W0201: Attribute 'split_image_folder_input' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:158:8: W0201: Attribute 'capture_region_window_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:162:8: W0201: Attribute 'image_loop_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:165:8: W0201: Attribute 'similarity_viewer_groupbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:169:8: W0201: Attribute 'table_live_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:173:8: W0201: Attribute 'table_highest_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:177:8: W0201: Attribute 'table_threshold_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:181:8: W0201: Attribute 'line_1' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:186:8: W0201: Attribute 'table_current_image_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:189:8: W0201: Attribute 'table_reset_image_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:192:8: W0201: Attribute 'line_2' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:197:8: W0201: Attribute 'line_3' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:202:8: W0201: Attribute 'line_4' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:207:8: W0201: Attribute 'line_5' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:212:8: W0201: Attribute 'table_current_image_live_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:216:8: W0201: Attribute 'table_current_image_highest_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:220:8: W0201: Attribute 'table_current_image_threshold_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:224:8: W0201: Attribute 'table_reset_image_live_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:228:8: W0201: Attribute 'table_reset_image_highest_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:232:8: W0201: Attribute 'table_reset_image_threshold_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:236:8: W0201: Attribute 'reload_start_image_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:240:8: W0201: Attribute 'start_image_status_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:243:8: W0201: Attribute 'start_image_status_value_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:246:8: W0201: Attribute 'image_loop_value_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:249:8: W0201: Attribute 'previous_image_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:254:8: W0201: Attribute 'next_image_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:296:8: W0201: Attribute 'menu_bar' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:299:8: W0201: Attribute 'menu_help' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:301:8: W0201: Attribute 'menu_file' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:304:8: W0201: Attribute 'action_view_help' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:306:8: W0201: Attribute 'action_about' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:308:8: W0201: Attribute 'actionSplit_Settings' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:310:8: W0201: Attribute 'action_save_profile' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:312:8: W0201: Attribute 'action_load_profile' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:314:8: W0201: Attribute 'action_save_profile_as' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:316:8: W0201: Attribute 'action_check_for_updates' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:318:8: W0201: Attribute 'actionCheck_for_Updates_on_Open' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:323:8: W0201: Attribute 'actionLoop_Last_Split_Image_To_First_Image' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:325:8: W0201: Attribute 'actionAuto_Start_On_Reset' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:327:8: W0201: Attribute 'actionGroup_dummy_splits_when_undoing_skipping' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:329:8: W0201: Attribute 'action_settings' defined outside __init__ (attribute-defined-outside-init) src\\gen\\design.py:331:8: W0201: Attribute 'action_check_for_updates_on_open' defined outside __init__ (attribute-defined-outside-init) ************* Module resources_rc src\\gen\\resources_rc.py:1:0: C0302: Too many lines in module (2311/1000) (too-many-lines) src\\gen\\resources_rc.py:8:0: C0103: Constant name \"qt_resource_data\" doesn't conform to UPPER_CASE naming style (invalid-name) src\\gen\\resources_rc.py:2278:0: C0103: Constant name \"qt_resource_name\" doesn't conform to UPPER_CASE naming style (invalid-name) src\\gen\\resources_rc.py:2294:0: C0103: Constant name \"qt_resource_struct\" doesn't conform to UPPER_CASE naming style (invalid-name) src\\gen\\resources_rc.py:2305:0: C0103: Function name \"qInitResources\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\resources_rc.py:2308:0: C0103: Function name \"qCleanupResources\" doesn't conform to snake_case naming style (invalid-name) ************* Module settings src\\gen\\settings.py:2:0: R2044: Line with empty comment (empty-comment) src\\gen\\settings.py:4:0: R2044: Line with empty comment (empty-comment) src\\gen\\settings.py:61:0: C0301: Line too long (158/120) (line-too-long) src\\gen\\settings.py:123:0: C0301: Line too long (151/120) (line-too-long) src\\gen\\settings.py:209:0: C0301: Line too long (162/120) (line-too-long) src\\gen\\settings.py:214:0: C0301: Line too long (121/120) (line-too-long) src\\gen\\settings.py:221:0: C0301: Line too long (177/120) (line-too-long) src\\gen\\settings.py:223:0: C0301: Line too long (181/120) (line-too-long) src\\gen\\settings.py:226:0: C0301: Line too long (461/120) (line-too-long) src\\gen\\settings.py:228:0: C0301: Line too long (192/120) (line-too-long) src\\gen\\settings.py:12:0: C0103: Class name \"Ui_DialogSettings\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name) src\\gen\\settings.py:12:0: R0205: Class 'Ui_DialogSettings' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance) src\\gen\\settings.py:12:0: R0902: Too many instance attributes (35/15) (too-many-instance-attributes) src\\gen\\settings.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\settings.py:13:22: C0103: Argument name \"DialogSettings\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\settings.py:16:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\settings.py:13:4: R0915: Too many statements (190/50) (too-many-statements) src\\gen\\settings.py:205:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\settings.py:205:28: C0103: Argument name \"DialogSettings\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\settings.py:26:8: W0201: Attribute 'capture_settings_groupbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:29:8: W0201: Attribute 'fps_limit_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:36:8: W0201: Attribute 'fps_limit_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:40:8: W0201: Attribute 'live_capture_region_checkbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:46:8: W0201: Attribute 'capture_method_combobox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:49:8: W0201: Attribute 'capture_method_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:52:8: W0201: Attribute 'capture_device_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:55:8: W0201: Attribute 'capture_device_combobox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:59:8: W0201: Attribute 'image_settings_groupbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:65:8: W0201: Attribute 'default_comparison_method' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:73:8: W0201: Attribute 'default_comparison_method_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:76:8: W0201: Attribute 'default_pause_time_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:80:8: W0201: Attribute 'default_pause_time_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:87:8: W0201: Attribute 'default_similarity_threshold_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:92:8: W0201: Attribute 'default_similarity_threshold_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:98:8: W0201: Attribute 'loop_splits_checkbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:104:8: W0201: Attribute 'custom_image_settings_info_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:111:8: W0201: Attribute 'default_delay_time_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:116:8: W0201: Attribute 'default_delay_time_spinbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:121:8: W0201: Attribute 'hotkeys_groupbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:127:8: W0201: Attribute 'set_pause_hotkey_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:131:8: W0201: Attribute 'split_input' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:137:8: W0201: Attribute 'undo_split_input' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:143:8: W0201: Attribute 'split_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:146:8: W0201: Attribute 'reset_input' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:152:8: W0201: Attribute 'set_undo_split_hotkey_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:156:8: W0201: Attribute 'reset_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:159:8: W0201: Attribute 'set_reset_hotkey_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:163:8: W0201: Attribute 'set_split_hotkey_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:167:8: W0201: Attribute 'pause_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:170:8: W0201: Attribute 'pause_input' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:176:8: W0201: Attribute 'undo_split_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:179:8: W0201: Attribute 'set_skip_split_hotkey_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:183:8: W0201: Attribute 'skip_split_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\settings.py:186:8: W0201: Attribute 'skip_split_input' defined outside __init__ (attribute-defined-outside-init) ************* Module update_checker src\\gen\\update_checker.py:2:0: R2044: Line with empty comment (empty-comment) src\\gen\\update_checker.py:4:0: R2044: Line with empty comment (empty-comment) src\\gen\\update_checker.py:12:0: C0103: Class name \"Ui_UpdateChecker\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name) src\\gen\\update_checker.py:12:0: R0205: Class 'Ui_UpdateChecker' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance) src\\gen\\update_checker.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\update_checker.py:13:22: C0103: Argument name \"UpdateChecker\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\update_checker.py:17:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\update_checker.py:33:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\update_checker.py:13:4: R0915: Too many statements (56/50) (too-many-statements) src\\gen\\update_checker.py:71:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\update_checker.py:71:28: C0103: Argument name \"UpdateChecker\" doesn't conform to snake_case naming style (invalid-name) src\\gen\\update_checker.py:31:8: W0201: Attribute 'update_status_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:39:8: W0201: Attribute 'current_version_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:42:8: W0201: Attribute 'latest_version_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:45:8: W0201: Attribute 'go_to_download_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:48:8: W0201: Attribute 'left_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:52:8: W0201: Attribute 'right_button' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:55:8: W0201: Attribute 'current_version_number_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:59:8: W0201: Attribute 'latest_version_number_label' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:63:8: W0201: Attribute 'do_not_ask_again_checkbox' defined outside __init__ (attribute-defined-outside-init) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (region_capture -> region_selection) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (error_messages -> user_profile -> region_capture -> region_selection) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplitImage -> split_parser) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoControlledWorker -> error_messages -> AutoSplit) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> user_profile -> region_capture -> region_selection -> error_messages) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplitImage -> error_messages -> user_profile) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> menu_bar -> user_profile -> region_capture -> region_selection -> error_messages) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> region_selection -> error_messages) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> error_messages) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (error_messages -> user_profile -> region_selection) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (error_messages -> user_profile) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplitImage -> split_parser -> error_messages -> user_profile) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> menu_bar -> region_selection -> error_messages) (cyclic-import) src\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> menu_bar -> error_messages) (cyclic-import) -------------------------------------------------------------------------- Your code has been rated at -158.32/10 (previous run: -285.20/10, +126.88) ``` ### Expected behavior src\\gen\\* should not be checked ### Pylint version ```shell pylint 2.14.1 astroid 2.11.5 Python 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)] ``` ### OS / Environment Windows 10.0.19044 ### Additional dependencies _No response_","hints_text":"@matusvalo Didn't you fix this recently? Or was this a case we overlooked? https://github.com/PyCQA/pylint/pull/6528. I will check I am not able to replicate the issue: ``` (pylint39) matusg@MacBook-Pro:~/dev/pylint/test$ cat src/gen/test.py import bla (pylint39) matusg@MacBook-Pro:~/dev/pylint/test$ pylint --version pylint 2.14.1 astroid 2.11.6 Python 3.9.12 (main, May 8 2022, 18:05:13) [Clang 12.0.0 (clang-1200.0.32.29)] (pylint39) matusg@MacBook-Pro:~/dev/pylint/test$ cat pyproject.toml [tool.pylint.MASTER] ignore-paths = [ # Auto generated \"^src/gen/.*$\", ] (pylint39) matusg@MacBook-Pro:~/dev/pylint/test$ pylint --recursive=y src/ (pylint39) matusg@MacBook-Pro:~/dev/pylint/test$ ``` I cannot verify the issue on windows. > NOTE: Commenting out `\"^src/gen/.*$\",` is yielding pylint errors in `test.py` file, so I consider that `ignore-paths` configuration is applied. @Avasam could you provide simple reproducer for the issue? > @Avasam could you provide simple reproducer for the issue? I too thought this was fixed by #6528. I'll try to come up with a simple repro. In the mean time, this is my project in question: https://github.com/Avasam/Auto-Split/tree/camera-capture-split-cam-option @matusvalo I think I've run into a similar (or possibly the same) issue. Trying to reproduce with your example: ``` % cat src/gen/test.py import bla % pylint --version pylint 2.13.9 astroid 2.11.5 Python 3.9.13 (main, May 24 2022, 21:28:31) [Clang 13.1.6 (clang-1316.0.21.2)] % cat pyproject.toml [tool.pylint.MASTER] ignore-paths = [ # Auto generated \"^src/gen/.*$\", ] ## Succeeds as expected % pylint --recursive=y src/ ## Fails for some reason % pylint --recursive=y . ************* Module test src/gen/test.py:1:0: C0114: Missing module docstring (missing-module-docstring) src/gen/test.py:1:0: E0401: Unable to import 'bla' (import-error) src/gen/test.py:1:0: W0611: Unused import bla (unused-import) ------------------------------------------------------------------ ``` EDIT: Just upgraded to 2.14.3, and still seems to report the same. Hmm I can reproduce your error, and now I understand the root cause. The root cause is following. The decision of skipping the path is here: https://github.com/PyCQA/pylint/blob/3c5eca2ded3dd2b59ebaf23eb289453b5d2930f0/pylint/lint/pylinter.py#L600-L607 * When you execute pylint with `src/` argument following variables are present: ```python (Pdb) p root 'src/gen' (Pdb) p self.config.ignore_paths [re.compile('^src\\\\\\\\gen\\\\\\\\.*$|^src/gen/.*$')] ``` * When you uexecute pylint with `.` argument following variables are present: ```python (Pdb) p root './src/gen' (Pdb) p self.config.ignore_paths [re.compile('^src\\\\\\\\gen\\\\\\\\.*$|^src/gen/.*$')] ``` In the second case, the source is prefixed with `./` which causes that path is not matched. The simple fix should be to use `os.path.normpath()` https://docs.python.org/3/library/os.path.html#os.path.normpath"},"metadata":{"created_at":"2022-06-28T17:24:43Z","version":"2.15","environment_setup_commit":"e90702074e68e20dc8e5df5013ee3ecf22139c3e"}}
{"task_id":"swebench_pylint_dev_pylint_7114","source":"swebench-lite","input":{"repo":"pylint-dev/pylint","instance_id":"pylint-dev__pylint-7114","base_commit":"397c1703e8ae6349d33f7b99f45b2ccaf581e666","prompt":"Linting fails if module contains module of the same name ### Steps to reproduce Given multiple files: ``` . `-- a/ |-- a.py `-- b.py ``` Which are all empty, running `pylint a` fails: ``` $ pylint a ************* Module a a/__init__.py:1:0: F0010: error while code parsing: Unable to load file a/__init__.py: [Errno 2] No such file or directory: 'a/__init__.py' (parse-error) $ ``` However, if I rename `a.py`, `pylint a` succeeds: ``` $ mv a/a.py a/c.py $ pylint a $ ``` Alternatively, I can also `touch a/__init__.py`, but that shouldn't be necessary anymore. ### Current behavior Running `pylint a` if `a/a.py` is present fails while searching for an `__init__.py` file. ### Expected behavior Running `pylint a` if `a/a.py` is present should succeed. ### pylint --version output Result of `pylint --version` output: ``` pylint 3.0.0a3 astroid 2.5.6 Python 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0] ``` ### Additional info This also has some side-effects in module resolution. For example, if I create another file `r.py`: ``` . |-- a | |-- a.py | `-- b.py `-- r.py ``` With the content: ``` from a import b ``` Running `pylint -E r` will run fine, but `pylint -E r a` will fail. Not just for module a, but for module r as well. ``` ************* Module r r.py:1:0: E0611: No name 'b' in module 'a' (no-name-in-module) ************* Module a a/__init__.py:1:0: F0010: error while code parsing: Unable to load file a/__init__.py: [Errno 2] No such file or directory: 'a/__init__.py' (parse-error) ``` Again, if I rename `a.py` to `c.py`, `pylint -E r a` will work perfectly.","hints_text":"@iFreilicht thanks for your report. #4909 was a duplicate."},"metadata":{"created_at":"2022-07-03T04:36:40Z","version":"2.15","environment_setup_commit":"e90702074e68e20dc8e5df5013ee3ecf22139c3e"}}
{"task_id":"swebench_pylint_dev_pylint_7228","source":"swebench-lite","input":{"repo":"pylint-dev/pylint","instance_id":"pylint-dev__pylint-7228","base_commit":"d597f252915ddcaaa15ccdfcb35670152cb83587","prompt":"rxg include '\\p{Han}' will throw error ### Bug description config rxg in pylintrc with \\p{Han} will throw err ### Configuration .pylintrc: ```ini function-rgx=[\\p{Han}a-z_][\\p{Han}a-z0-9_]{2,30}$ ``` ### Command used ```shell pylint ``` ### Pylint output ```shell (venvtest) tsung-hande-MacBook-Pro:robot_is_comming tsung-han$ pylint Traceback (most recent call last): File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/bin/pylint\", line 8, in <module> sys.exit(run_pylint()) File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/__init__.py\", line 25, in run_pylint PylintRun(argv or sys.argv[1:]) File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/lint/run.py\", line 161, in __init__ args = _config_initialization( File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/config/config_initialization.py\", line 57, in _config_initialization linter._parse_configuration_file(config_args) File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/config/arguments_manager.py\", line 244, in _parse_configuration_file self.config, parsed_args = self._arg_parser.parse_known_args( File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 1858, in parse_known_args namespace, args = self._parse_known_args(args, namespace) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2067, in _parse_known_args start_index = consume_optional(start_index) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2007, in consume_optional take_action(action, args, option_string) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 1919, in take_action argument_values = self._get_values(action, argument_strings) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2450, in _get_values value = self._get_value(action, arg_string) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2483, in _get_value result = type_func(arg_string) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\", line 252, in compile return _compile(pattern, flags) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\", line 304, in _compile p = sre_compile.compile(pattern, flags) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 788, in compile p = sre_parse.parse(p, flags) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 955, in parse p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 444, in _parse_sub itemsappend(_parse(source, state, verbose, nested + 1, File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 555, in _parse code1 = _class_escape(source, this) File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 350, in _class_escape raise source.error('bad escape %s' % escape, len(escape)) re.error: bad escape \\p at position 1 ``` ### Expected behavior not throw error ### Pylint version ```shell pylint 2.14.4 astroid 2.11.7 Python 3.9.13 (main, May 24 2022, 21:28:44) [Clang 13.0.0 (clang-1300.0.29.30)] ``` ### OS / Environment macOS 11.6.7","hints_text":"This doesn't seem like it is a `pylint` issue? `re.compile(\"[\\p{Han}a-z_]\")` also raises normally. `\\p` also isn't documented: https://docs.python.org/3/howto/regex.html Is this a supported character? I think this could be improved! Similar to the helpful output when passing an unrecognized option to Pylint, we could give a friendly output indicating that the regex pattern is invalid without the traceback; happy to put a MR together if you agree. Thanks @mbyrnepr2 I did not even realize it was a crash that we had to fix before your comment. @mbyrnepr2 I think in the above stacktrace on line 1858 makes the most sense. We need to decide though if we continue to run the program. I think it makes sense to still quit. If we catch regex errors there and pass we will also \"allow\" ignore path regexes that don't work. I don't think we should do that. Imo, incorrect regexes are a little different from other \"incorrect\" options, since there is little risk that they are valid on other interpreters or versions such as old messages etc. Therefore, I'd prefer to (cleanly) exit. Indeed @DanielNoord I think we are on the same page regarding this point; I would also exit instead of passing if the regex is invalid. That line you mention, we can basically try/except on re.error and exit printing the details of the pattern which is invalid."},"metadata":{"created_at":"2022-07-25T17:19:11Z","version":"2.15","environment_setup_commit":"e90702074e68e20dc8e5df5013ee3ecf22139c3e"}}
{"task_id":"swebench_pylint_dev_pylint_7993","source":"swebench-lite","input":{"repo":"pylint-dev/pylint","instance_id":"pylint-dev__pylint-7993","base_commit":"e90702074e68e20dc8e5df5013ee3ecf22139c3e","prompt":"Using custom braces in message template does not work ### Bug description Have any list of errors: On pylint 1.7 w/ python3.6 - I am able to use this as my message template ``` $ pylint test.py --msg-template='{{ \"Category\": \"{category}\" }}' No config file found, using default configuration ************* Module [redacted].test { \"Category\": \"convention\" } { \"Category\": \"error\" } { \"Category\": \"error\" } { \"Category\": \"convention\" } { \"Category\": \"convention\" } { \"Category\": \"convention\" } { \"Category\": \"error\" } ``` However, on Python3.9 with Pylint 2.12.2, I get the following: ``` $ pylint test.py --msg-template='{{ \"Category\": \"{category}\" }}' [redacted]/site-packages/pylint/reporters/text.py:206: UserWarning: Don't recognize the argument '{ \"Category\"' in the --msg-template. Are you sure it is supported on the current version of pylint? warnings.warn( ************* Module [redacted].test \" } \" } \" } \" } \" } \" } ``` Is this intentional or a bug? ### Configuration _No response_ ### Command used ```shell pylint test.py --msg-template='{{ \"Category\": \"{category}\" }}' ``` ### Pylint output ```shell [redacted]/site-packages/pylint/reporters/text.py:206: UserWarning: Don't recognize the argument '{ \"Category\"' in the --msg-template. Are you sure it is supported on the current version of pylint? warnings.warn( ************* Module [redacted].test \" } \" } \" } \" } \" } \" } ``` ### Expected behavior Expect the dictionary to print out with `\"Category\"` as the key. ### Pylint version ```shell Affected Version: pylint 2.12.2 astroid 2.9.2 Python 3.9.9+ (heads/3.9-dirty:a2295a4, Dec 21 2021, 22:32:52) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] Previously working version: No config file found, using default configuration pylint 1.7.4, astroid 1.6.6 Python 3.6.8 (default, Nov 16 2020, 16:55:22) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] ``` ### OS / Environment _No response_ ### Additional dependencies _No response_","hints_text":"Subsequently, there is also this behavior with the quotes ``` $ pylint test.py --msg-template='\"Category\": \"{category}\"' ************* Module test Category\": \"convention Category\": \"error Category\": \"error Category\": \"convention Category\": \"convention Category\": \"error $ pylint test.py --msg-template='\"\"Category\": \"{category}\"\"' ************* Module test \"Category\": \"convention\" \"Category\": \"error\" \"Category\": \"error\" \"Category\": \"convention\" \"Category\": \"convention\" \"Category\": \"error\" ``` Commit that changed the behavior was probably this one: https://github.com/PyCQA/pylint/commit/7c3533ca48e69394391945de1563ef7f639cd27d#diff-76025f0bc82e83cb406321006fbca12c61a10821834a3164620fc17c978f9b7e And I tested on 2.11.1 that it is working as intended on that version. Thanks for digging into this !"},"metadata":{"created_at":"2022-12-27T18:20:50Z","version":"2.15","environment_setup_commit":"e90702074e68e20dc8e5df5013ee3ecf22139c3e"}}
{"task_id":"swebench_pytest_dev_pytest_11143","source":"swebench-lite","input":{"repo":"pytest-dev/pytest","instance_id":"pytest-dev__pytest-11143","base_commit":"6995257cf470d2143ad1683824962de4071c0eb7","prompt":"Rewrite fails when first expression of file is a number and mistaken as docstring <!-- Thanks for submitting an issue! Quick check-list while reporting bugs: --> - [x] a detailed description of the bug or problem you are having - [x] output of `pip list` from the virtual environment you are using - [x] pytest and operating system versions - [x] minimal example if possible ``` Installing collected packages: zipp, six, PyYAML, python-dateutil, MarkupSafe, importlib-metadata, watchdog, tomli, soupsieve, pyyaml-env-tag, pycparser, pluggy, packaging, mergedeep, Markdown, jinja2, iniconfig, ghp-import, exceptiongroup, click, websockets, urllib3, tqdm, smmap, pytest, pyee, mkdocs, lxml, importlib-resources, idna, cssselect, charset-normalizer, cffi, certifi, beautifulsoup4, attrs, appdirs, w3lib, typing-extensions, texttable, requests, pyzstd, pytest-metadata, pyquery, pyppmd, pyppeteer, pynacl, pymdown-extensions, pycryptodomex, pybcj, pyasn1, py, psutil, parse, multivolumefile, mkdocs-autorefs, inflate64, gitdb, fake-useragent, cryptography, comtypes, bs4, brotli, bcrypt, allure-python-commons, xlwt, xlrd, rsa, requests-html, pywinauto, python-i18n, python-dotenv, pytest-rerunfailures, pytest-html, pytest-check, PySocks, py7zr, paramiko, mkdocstrings, loguru, GitPython, ftputil, crcmod, chardet, brotlicffi, allure-pytest Successfully installed GitPython-3.1.31 Markdown-3.3.7 MarkupSafe-2.1.3 PySocks-1.7.1 PyYAML-6.0 allure-pytest-2.13.2 allure-python-commons-2.13.2 appdirs-1.4.4 attrs-23.1.0 bcrypt-4.0.1 beautifulsoup4-4.12.2 brotli-1.0.9 brotlicffi-1.0.9.2 bs4-0.0.1 certifi-2023.5.7 cffi-1.15.1 chardet-5.1.0 charset-normalizer-3.1.0 click-8.1.3 comtypes-1.2.0 crcmod-1.7 cryptography-41.0.1 cssselect-1.2.0 exceptiongroup-1.1.1 fake-useragent-1.1.3 ftputil-5.0.4 ghp-import-2.1.0 gitdb-4.0.10 idna-3.4 importlib-metadata-6.7.0 importlib-resources-5.12.0 inflate64-0.3.1 iniconfig-2.0.0 jinja2-3.1.2 loguru-0.7.0 lxml-4.9.2 mergedeep-1.3.4 mkdocs-1.4.3 mkdocs-autorefs-0.4.1 mkdocstrings-0.22.0 multivolumefile-0.2.3 packaging-23.1 paramiko-3.2.0 parse-1.19.1 pluggy-1.2.0 psutil-5.9.5 py-1.11.0 py7zr-0.20.5 pyasn1-0.5.0 pybcj-1.0.1 pycparser-2.21 pycryptodomex-3.18.0 pyee-8.2.2 pymdown-extensions-10.0.1 pynacl-1.5.0 pyppeteer-1.0.2 pyppmd-1.0.0 pyquery-2.0.0 pytest-7.4.0 pytest-check-2.1.5 pytest-html-3.2.0 pytest-metadata-3.0.0 pytest-rerunfailures-11.1.2 python-dateutil-2.8.2 python-dotenv-1.0.0 python-i18n-0.3.9 pywinauto-0.6.6 pyyaml-env-tag-0.1 pyzstd-0.15.9 requests-2.31.0 requests-html-0.10.0 rsa-4.9 six-1.16.0 smmap-5.0.0 soupsieve-2.4.1 texttable-1.6.7 tomli-2.0.1 tqdm-4.65.0 typing-extensions-4.6.3 urllib3-1.26.16 w3lib-2.1.1 watchdog-3.0.0 websockets-10.4 xlrd-2.0.1 xlwt-1.3.0 zipp-3.15.0 ``` use `pytest -k xxx`ï¼Œ report an errorï¼š`TypeError: argument of type 'int' is not iterable` it seems a error in collecting testcase ``` ==================================== ERRORS ==================================== _ ERROR collecting testcases/åŸºçº¿/ä»£ç†ç­–ç•¥/SOCKSäºŒçº§ä»£ç†è¿­ä»£äºŒ/åœ¨çº¿ç”¨æˆ·/åœ¨çº¿ç”¨æˆ·æ›´æ–°/ä¸Šçº¿ç”¨æˆ·/test_socks_user_011.py _ /usr/local/lib/python3.8/site-packages/_pytest/runner.py:341: in from_call result: Optional[TResult] = func() /usr/local/lib/python3.8/site-packages/_pytest/runner.py:372: in <lambda> call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\") /usr/local/lib/python3.8/site-packages/_pytest/python.py:531: in collect self._inject_setup_module_fixture() /usr/local/lib/python3.8/site-packages/_pytest/python.py:545: in _inject_setup_module_fixture self.obj, (\"setUpModule\", \"setup_module\") /usr/local/lib/python3.8/site-packages/_pytest/python.py:310: in obj self._obj = obj = self._getobj() /usr/local/lib/python3.8/site-packages/_pytest/python.py:528: in _getobj return self._importtestmodule() /usr/local/lib/python3.8/site-packages/_pytest/python.py:617: in _importtestmodule mod = import_path(self.path, mode=importmode, root=self.config.rootpath) /usr/local/lib/python3.8/site-packages/_pytest/pathlib.py:565: in import_path importlib.import_module(module_name) /usr/local/lib/python3.8/importlib/__init__.py:127: in import_module return _bootstrap._gcd_import(name[level:], package, level) <frozen importlib._bootstrap>:1014: in _gcd_import ??? <frozen importlib._bootstrap>:991: in _find_and_load ??? <frozen importlib._bootstrap>:975: in _find_and_load_unlocked ??? <frozen importlib._bootstrap>:671: in _load_unlocked ??? /usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:169: in exec_module source_stat, co = _rewrite_test(fn, self.config) /usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:352: in _rewrite_test rewrite_asserts(tree, source, strfn, config) /usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:413: in rewrite_asserts AssertionRewriter(module_path, config, source).run(mod) /usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:695: in run if self.is_rewrite_disabled(doc): /usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:760: in is_rewrite_disabled return \"PYTEST_DONT_REWRITE\" in docstring E TypeError: argument of type 'int' is not iterable ```","hints_text":"more details are needed - based on the exception, the docstring is a integer, that seems completely wrong I run it pass lasttime in 2023-6-20 17:07:23. it run in docker and install newest pytest before run testcase everytime . maybe some commit cause it recently. I run it can pass in 7.2.0 a few minutes ago. `pytest ini` ``` [pytest] log_cli = false log_cli_level = debug log_cli_format = %(asctime)s %(levelname)s %(message)s log_cli_date_format = %Y-%m-%d %H:%M:%S addopts = -v -s filterwarnings = ignore::UserWarning markers= case_id: mark test id to upload on tp case_level_bvt: testcase level bvt case_level_1: testcase level level 1 case_level_2: testcase level level 2 case_level_3: testcase level level 3 case_status_pass: mark case as PASS case_status_fail: mark case as FAILED case_status_not_finish: mark case as CODEING case_status_not_run: mark case as FINISH case_not_run: mark case as DONT RUN run_env: mark run this case on which environment ``` `testcase:` ``` @pytest.fixture(autouse=True) def default_setup_teardown(): xxxx @allure.feature(\"åˆå§‹çŠ¶æ€\") class TestDefauleName: @allure.title(\"ä¸Šçº¿ä¸€ä¸ªåŸŸç”¨æˆ·ï¼Œç”¨æˆ·åå’Œç»„åæ­£ç¡®\") @pytest.mark.case_level_1 @pytest.mark.case_id(\"tc_proxyheard_insert_011\") def test_tc_proxyheard_insert_011(self): xxxx ``` thanks for the update i took the liberty to edit your comments to use markdown code blocks for ease of reading from the given information the problem is still unclear please try running with `--assert=plain` for verification the error indicates that the python ast parser somehow ends up with a integer as the docstring for `test_socks_user_011.py` the reason is still unclear based on the redacted information I run with --assert=plain and it has passed python3 -m pytest -k helloworld --assert=plain ``` testcases/smoke_testcase/test_helloworld.py::TestGuardProcess::test_hello_world 2023-06-25 08:54:17.659 | INFO | NAC_AIO.testcases.smoke_testcase.test_helloworld:test_hello_world:15 - Great! Frame Work is working PASSED total: 1648 passed: 1 failed: 0 error: 0 pass_rate 100.00% ================================================================================= 1 passed, 1647 deselected in 12.28s ================================================================================= ``` It seems to me that we have a potential bug in the ast transformer where's in case the first expression of a file is a integer, we mistake it as a docstring Can you verify the first expression in the file that fails? you are right this file first expression is a 0 . It can pass after I delete it thank you! Minimal reproducer: ```python 0 ``` (yes, just that, in a .py file)"},"metadata":{"created_at":"2023-06-26T06:44:43Z","version":"8.0","environment_setup_commit":"10056865d2a4784934ce043908a0e78d0578f677"}}
{"task_id":"swebench_pytest_dev_pytest_11148","source":"swebench-lite","input":{"repo":"pytest-dev/pytest","instance_id":"pytest-dev__pytest-11148","base_commit":"2f7415cfbc4b6ca62f9013f1abd27136f46b9653","prompt":"Module imported twice under import-mode=importlib In pmxbot/pmxbot@7f189ad, I'm attempting to switch pmxbot off of pkg_resources style namespace packaging to PEP 420 namespace packages. To do so, I've needed to switch to `importlib` for the `import-mode` and re-organize the tests to avoid import errors on the tests. Yet even after working around these issues, the tests are failing when the effect of `core.initialize()` doesn't seem to have had any effect. Investigating deeper, I see that initializer is executed and performs its actions (setting a class variable `pmxbot.logging.Logger.store`), but when that happens, there are two different versions of `pmxbot.logging` present, one in `sys.modules` and another found in `tests.unit.test_commands.logging`: ``` =========================================================================== test session starts =========================================================================== platform darwin -- Python 3.11.1, pytest-7.2.0, pluggy-1.0.0 cachedir: .tox/python/.pytest_cache rootdir: /Users/jaraco/code/pmxbot/pmxbot, configfile: pytest.ini plugins: black-0.3.12, mypy-0.10.3, jaraco.test-5.3.0, checkdocs-2.9.0, flake8-1.1.1, enabler-2.0.0, jaraco.mongodb-11.2.1, pmxbot-1122.14.3.dev13+g7f189ad collected 421 items / 180 deselected / 241 selected run-last-failure: rerun previous 240 failures (skipped 14 files) tests/unit/test_commands.py E >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> cls = <class 'tests.unit.test_commands.TestCommands'> @classmethod def setup_class(cls): path = os.path.dirname(os.path.abspath(__file__)) configfile = os.path.join(path, 'testconf.yaml') config = pmxbot.dictlib.ConfigDict.from_yaml(configfile) cls.bot = core.initialize(config) > logging.Logger.store.message(\"logged\", \"testrunner\", \"some text\") E AttributeError: type object 'Logger' has no attribute 'store' tests/unit/test_commands.py:37: AttributeError >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> > /Users/jaraco/code/pmxbot/pmxbot/tests/unit/test_commands.py(37)setup_class() -> logging.Logger.store.message(\"logged\", \"testrunner\", \"some text\") (Pdb) logging.Logger <class 'pmxbot.logging.Logger'> (Pdb) logging <module 'pmxbot.logging' from '/Users/jaraco/code/pmxbot/pmxbot/pmxbot/logging.py'> (Pdb) import sys (Pdb) sys.modules['pmxbot.logging'] <module 'pmxbot.logging' from '/Users/jaraco/code/pmxbot/pmxbot/pmxbot/logging.py'> (Pdb) sys.modules['pmxbot.logging'] is logging False ``` I haven't yet made a minimal reproducer, but I wanted to first capture this condition.","hints_text":"In pmxbot/pmxbot@3adc54c, I've managed to pare down the project to a bare minimum reproducer. The issue only happens when `import-mode=importlib` and `doctest-modules` and one of the modules imports another module. This issue may be related to (or same as) #10341. I think you'll agree this is pretty basic behavior that should be supported. I'm not even aware of a good workaround. Hey @jaraco, thanks for the reproducer! I found the problem, will open a PR shortly."},"metadata":{"created_at":"2023-06-29T00:04:33Z","version":"8.0","environment_setup_commit":"10056865d2a4784934ce043908a0e78d0578f677"}}
{"task_id":"swebench_pytest_dev_pytest_5103","source":"swebench-lite","input":{"repo":"pytest-dev/pytest","instance_id":"pytest-dev__pytest-5103","base_commit":"10ca84ffc56c2dd2d9dc4bd71b7b898e083500cd","prompt":"Unroll the iterable for all/any calls to get better reports Sometime I need to assert some predicate on all of an iterable, and for that the builtin functions `all`/`any` are great - but the failure messages aren't useful at all! For example - the same test written in three ways: - A generator expression ```sh def test_all_even(): even_stevens = list(range(1,100,2)) > assert all(is_even(number) for number in even_stevens) E assert False E + where False = all(<generator object test_all_even.<locals>.<genexpr> at 0x101f82ed0>) ``` - A list comprehension ```sh def test_all_even(): even_stevens = list(range(1,100,2)) > assert all([is_even(number) for number in even_stevens]) E assert False E + where False = all([False, False, False, False, False, False, ...]) ``` - A for loop ```sh def test_all_even(): even_stevens = list(range(1,100,2)) for number in even_stevens: > assert is_even(number) E assert False E + where False = is_even(1) test_all_any.py:7: AssertionError ``` The only one that gives a meaningful report is the for loop - but it's way more wordy, and `all` asserts don't translate to a for loop nicely (I'll have to write a `break` or a helper function - yuck) I propose the assertion re-writer \"unrolls\" the iterator to the third form, and then uses the already existing reports. - [x] Include a detailed description of the bug or suggestion - [x] `pip list` of the virtual environment you are using ``` Package Version -------------- ------- atomicwrites 1.3.0 attrs 19.1.0 more-itertools 7.0.0 pip 19.0.3 pluggy 0.9.0 py 1.8.0 pytest 4.4.0 setuptools 40.8.0 six 1.12.0 ``` - [x] pytest and operating system versions `platform darwin -- Python 3.7.3, pytest-4.4.0, py-1.8.0, pluggy-0.9.0` - [x] Minimal example if possible","hints_text":"Hello, I am new here and would be interested in working on this issue if that is possible. @danielx123 Sure! But I don't think this is an easy issue, since it involved the assertion rewriting - but if you're familar with Python's AST and pytest's internals feel free to pick this up. We also have a tag \"easy\" for issues that are probably easier for starting contributors: https://github.com/pytest-dev/pytest/issues?q=is%3Aopen+is%3Aissue+label%3A%22status%3A+easy%22 I was planning on starting a pr today, but probably won't be able to finish it until next week - @danielx123 maybe we could collaborate?"},"metadata":{"created_at":"2019-04-13T16:17:45Z","version":"4.5","environment_setup_commit":"693c3b7f61d4d32f8927a74f34ce8ac56d63958e"}}
{"task_id":"swebench_pytest_dev_pytest_5221","source":"swebench-lite","input":{"repo":"pytest-dev/pytest","instance_id":"pytest-dev__pytest-5221","base_commit":"4a2fdce62b73944030cff9b3e52862868ca9584d","prompt":"Display fixture scope with `pytest --fixtures` It would be useful to show fixture scopes with `pytest --fixtures`; currently the only way to learn the scope of a fixture is look at the docs (when that is documented) or at the source code."},"metadata":{"created_at":"2019-05-06T22:36:44Z","version":"4.4","environment_setup_commit":"4ccaa987d47566e3907f2f74167c4ab7997f622f"}}
{"task_id":"swebench_pytest_dev_pytest_5227","source":"swebench-lite","input":{"repo":"pytest-dev/pytest","instance_id":"pytest-dev__pytest-5227","base_commit":"2051e30b9b596e944524ccb787ed20f9f5be93e3","prompt":"Improve default logging format Currently it is: > DEFAULT_LOG_FORMAT = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\" I think `name` (module name) would be very useful here, instead of just the base filename. (It might also be good to have the relative path there (maybe at the end), but it is usually still very long (but e.g. `$VIRTUAL_ENV` could be substituted therein)) Currently it would look like this: ``` utils.py 114 DEBUG (0.000) SELECT \"app_url\".\"id\", \"app_url\".\"created\", \"app_url\".\"url\" FROM \"app_url\" WHERE \"app_url\".\"id\" = 2; args=(2,) multipart.py 604 DEBUG Calling on_field_start with no data ``` Using `DEFAULT_LOG_FORMAT = \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"` instead: ``` DEBUG django.db.backends:utils.py:114 (0.000) SELECT \"app_url\".\"id\", \"app_url\".\"created\", \"app_url\".\"url\" FROM \"app_url\" WHERE \"app_url\".\"id\" = 2; args=(2,) DEBUG multipart.multipart:multipart.py:604 Calling on_field_start with no data ```"},"metadata":{"created_at":"2019-05-07T20:27:24Z","version":"4.4","environment_setup_commit":"4ccaa987d47566e3907f2f74167c4ab7997f622f"}}
{"task_id":"swebench_pytest_dev_pytest_5413","source":"swebench-lite","input":{"repo":"pytest-dev/pytest","instance_id":"pytest-dev__pytest-5413","base_commit":"450d2646233c670654744d3d24330b69895bb9d2","prompt":"str() on the pytest.raises context variable doesn't behave same as normal exception catch Pytest 4.6.2, macOS 10.14.5 ```Python try: raise LookupError( f\"A\\n\" f\"B\\n\" f\"C\" ) except LookupError as e: print(str(e)) ``` prints > A > B > C But ```Python with pytest.raises(LookupError) as e: raise LookupError( f\"A\\n\" f\"B\\n\" f\"C\" ) print(str(e)) ``` prints > <console>:3: LookupError: A In order to get the full error message, one must do `str(e.value)`, which is documented, but this is a different interaction. Any chance the behavior could be changed to eliminate this gotcha? ----- Pip list gives ``` Package Version Location ------------------ -------- ------------------------------------------------------ apipkg 1.5 asn1crypto 0.24.0 atomicwrites 1.3.0 attrs 19.1.0 aws-xray-sdk 0.95 boto 2.49.0 boto3 1.9.51 botocore 1.12.144 certifi 2019.3.9 cffi 1.12.3 chardet 3.0.4 Click 7.0 codacy-coverage 1.3.11 colorama 0.4.1 coverage 4.5.3 cryptography 2.6.1 decorator 4.4.0 docker 3.7.2 docker-pycreds 0.4.0 docutils 0.14 ecdsa 0.13.2 execnet 1.6.0 future 0.17.1 idna 2.8 importlib-metadata 0.17 ipaddress 1.0.22 Jinja2 2.10.1 jmespath 0.9.4 jsondiff 1.1.1 jsonpickle 1.1 jsonschema 2.6.0 MarkupSafe 1.1.1 mock 3.0.4 more-itertools 7.0.0 moto 1.3.7 neobolt 1.7.10 neotime 1.7.4 networkx 2.1 numpy 1.15.0 packaging 19.0 pandas 0.24.2 pip 19.1.1 pluggy 0.12.0 prompt-toolkit 2.0.9 py 1.8.0 py2neo 4.2.0 pyaml 19.4.1 pycodestyle 2.5.0 pycparser 2.19 pycryptodome 3.8.1 Pygments 2.3.1 pyOpenSSL 19.0.0 pyparsing 2.4.0 pytest 4.6.2 pytest-cache 1.0 pytest-codestyle 1.4.0 pytest-cov 2.6.1 pytest-forked 1.0.2 python-dateutil 2.7.3 python-jose 2.0.2 pytz 2018.5 PyYAML 5.1 requests 2.21.0 requests-mock 1.5.2 responses 0.10.6 s3transfer 0.1.13 setuptools 41.0.1 six 1.11.0 sqlite3worker 1.1.7 tabulate 0.8.3 urllib3 1.24.3 wcwidth 0.1.7 websocket-client 0.56.0 Werkzeug 0.15.2 wheel 0.33.1 wrapt 1.11.1 xlrd 1.1.0 xmltodict 0.12.0 zipp 0.5.1 ```","hints_text":"> Any chance the behavior could be changed to eliminate this gotcha? What do you suggest? Proxying through to the exceptions `__str__`? Hi @fiendish, Indeed this is a bit confusing. Currently `ExceptionInfo` objects (which is `pytest.raises` returns to the context manager) implements `__str__` like this: https://github.com/pytest-dev/pytest/blob/9f8b566ea976df3a3ea16f74b56dd6d4909b84ee/src/_pytest/_code/code.py#L537-L542 I don't see much use for this, I would rather it didn't implement `__str__` at all and let `__repr__` take over, which would show something like: ``` <ExceptionInfo LookupError tb=10> ``` Which makes it more obvious that this is not what the user intended with `str(e)` probably. So I think a good solution is to simply delete the `__str__` method. Thoughts? Also, @fiendish which Python version are you using? > So I think a good solution is to simply delete the `__str__` method. Makes sense to me. Python 3.7.3 My ideal outcome would be for str(e) to act the same as str(e.value), but I can understand if that isn't desired. > My ideal outcome would be for str(e) to act the same as str(e.value), but I can understand if that isn't desired. I understand, but I think it is better to be explicit here, because users might use `print(e)` to see what `e` is, assume it is the exception value, and then get confused later when it actually isn't (an `isinstance` check or accessing `e.args`). +1 for deleting the current `__str__` implementation -1 for proxying it to the underlying `e.value` the `ExceptionInfo` object is not the exception and anything that makes it look more like the exception is just going to add to the confusion"},"metadata":{"created_at":"2019-06-06T15:21:20Z","version":"4.6","environment_setup_commit":"d5843f89d3c008ddcb431adbc335b080a79e617e"}}
{"task_id":"swebench_scikit_learn_scikit_learn_10297","source":"swebench-lite","input":{"repo":"scikit-learn/scikit-learn","instance_id":"scikit-learn__scikit-learn-10297","base_commit":"b90661d6a46aa3619d3eec94d5281f5888add501","prompt":"linear_model.RidgeClassifierCV's Parameter store_cv_values issue #### Description Parameter store_cv_values error on sklearn.linear_model.RidgeClassifierCV #### Steps/Code to Reproduce import numpy as np from sklearn import linear_model as lm #test database n = 100 x = np.random.randn(n, 30) y = np.random.normal(size = n) rr = lm.RidgeClassifierCV(alphas = np.arange(0.1, 1000, 0.1), normalize = True, store_cv_values = True).fit(x, y) #### Expected Results Expected to get the usual ridge regression model output, keeping the cross validation predictions as attribute. #### Actual Results TypeError: __init__() got an unexpected keyword argument 'store_cv_values' lm.RidgeClassifierCV actually has no parameter store_cv_values, even though some attributes depends on it. #### Versions Windows-10-10.0.14393-SP0 Python 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] NumPy 1.13.3 SciPy 0.19.1 Scikit-Learn 0.19.1 Add store_cv_values boolean flag support to RidgeClassifierCV Add store_cv_values support to RidgeClassifierCV - documentation claims that usage of this flag is possible: > cv_values_ : array, shape = [n_samples, n_alphas] or shape = [n_samples, n_responses, n_alphas], optional > Cross-validation values for each alpha (if **store_cv_values**=True and `cv=None`). While actually usage of this flag gives > TypeError: **init**() got an unexpected keyword argument 'store_cv_values'","hints_text":"thanks for the report. PR welcome. Can I give it a try? sure, thanks! please make the change and add a test in your pull request Can I take this? Thanks for the PR! LGTM @MechCoder review and merge? I suppose this should include a brief test... Indeed, please @yurii-andrieiev add a quick test to check that setting this parameter makes it possible to retrieve the cv values after a call to fit. @yurii-andrieiev do you want to finish this or have someone else take it over?"},"metadata":{"created_at":"2017-12-12T22:07:47Z","version":"0.20","environment_setup_commit":"55bf5d93e5674f13a1134d93a11fd0cd11aabcd1"}}
{"task_id":"swebench_scikit_learn_scikit_learn_10508","source":"swebench-lite","input":{"repo":"scikit-learn/scikit-learn","instance_id":"scikit-learn__scikit-learn-10508","base_commit":"c753b77ac49e72ebc0fe5e3c2369fe628f975017","prompt":"LabelEncoder transform fails for empty lists (for certain inputs) Python 3.6.3, scikit_learn 0.19.1 Depending on which datatypes were used to fit the LabelEncoder, transforming empty lists works or not. Expected behavior would be that empty arrays are returned in both cases. ```python >>> from sklearn.preprocessing import LabelEncoder >>> le = LabelEncoder() >>> le.fit([1,2]) LabelEncoder() >>> le.transform([]) array([], dtype=int64) >>> le.fit([\"a\",\"b\"]) LabelEncoder() >>> le.transform([]) Traceback (most recent call last): File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 57, in _wrapfunc return getattr(obj, method)(*args, **kwds) TypeError: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"[...]\\Python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py\", line 134, in transform return np.searchsorted(self.classes_, y) File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 1075, in searchsorted return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter) File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 67, in _wrapfunc return _wrapit(obj, method, *args, **kwds) File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 47, in _wrapit result = getattr(asarray(obj), method)(*args, **kwds) TypeError: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe' ```","hints_text":"`le.transform([])` will trigger an numpy array of `dtype=np.float64` and you fit something which was some string. ```python from sklearn.preprocessing import LabelEncoder import numpy as np le = LabelEncoder() X = np.array([\"a\", \"b\"]) le.fit(X) X_trans = le.transform(np.array([], dtype=X.dtype)) X_trans array([], dtype=int64) ``` I would like to take it up. Hey @maykulkarni go ahead with PR. Sorry, please don't mind my referenced commit, I don't intend to send in a PR. I would be happy to have a look over your PR once you send in (not that my review would matter much) :)"},"metadata":{"created_at":"2018-01-19T18:00:29Z","version":"0.20","environment_setup_commit":"55bf5d93e5674f13a1134d93a11fd0cd11aabcd1"}}
